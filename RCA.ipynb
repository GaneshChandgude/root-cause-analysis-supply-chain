{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uKMqE4Czx3Bv",
        "outputId": "4c0b73ea-d890-460e-e04d-df580b43b20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NnYL0eKBlkON",
        "outputId": "bf2a817f-91a4-42ed-8f56-787466006b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "pip install pydantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9FK4t0GUjMHT",
        "outputId": "0262f623-5a95-4917-cd4f-b8aaeaaebbb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.6)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.13.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.1)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "G-SirWV_jN4q",
        "outputId": "a1779ecc-1740-4ff2-b92e-4fce039c4a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.6)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.13.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Tk23f7BCjPfT",
        "outputId": "e96fa94c-948e-476e-e673-65fff54ee0e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.2.6)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (0.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.41.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jS_DacDNjQ7Z",
        "outputId": "1666231e-226e-4db6-963b-898312486e7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.7)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.6 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.2.6)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.14.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.13.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c7oypoPxjSco",
        "outputId": "f3f07464-3981-4e77-dc07-df3cfb45fe4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: groq<1.0.0,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.37.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (1.2.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.15.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (0.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq<1.0.0,>=0.30.0->langchain-groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jkniCEROjUj8",
        "outputId": "facdca46-3534-4472-d44f-4a4c99f082b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.5)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.2.6)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.1)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.12.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.13.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "pip install langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "X4SxguibjW7H",
        "outputId": "c6399dd5-2a51-463c-d266-8860e9c514d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k3WnNCGEUoUZ",
        "outputId": "eb1d6050-1e07-4901-a830-55bcde3d7f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langmem in /usr/local/lib/python3.12/dist-packages (0.0.30)\n",
            "Requirement already satisfied: langchain-anthropic>=0.3.3 in /usr/local/lib/python3.12/dist-packages (from langmem) (1.3.1)\n",
            "Requirement already satisfied: langchain-core>=0.3.46 in /usr/local/lib/python3.12/dist-packages (from langmem) (1.2.6)\n",
            "Requirement already satisfied: langchain-openai>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from langmem) (1.1.7)\n",
            "Requirement already satisfied: langchain>=0.3.15 in /usr/local/lib/python3.12/dist-packages (from langmem) (1.2.3)\n",
            "Requirement already satisfied: langgraph-checkpoint>=2.0.12 in /usr/local/lib/python3.12/dist-packages (from langmem) (3.0.1)\n",
            "Requirement already satisfied: langgraph<2,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from langmem) (1.0.5)\n",
            "Requirement already satisfied: langsmith>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from langmem) (0.6.1)\n",
            "Requirement already satisfied: trustcall>=0.0.39 in /usr/local/lib/python3.12/dist-packages (from langmem) (0.0.39)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.3.15->langmem) (2.12.3)\n",
            "Requirement already satisfied: anthropic<1.0.0,>=0.75.0 in /usr/local/lib/python3.12/dist-packages (from langchain-anthropic>=0.3.3->langmem) (0.76.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.46->langmem) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.46->langmem) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.46->langmem) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.46->langmem) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.46->langmem) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.46->langmem) (0.13.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai>=0.3.1->langmem) (2.14.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai>=0.3.1->langmem) (0.12.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<2,>=0.6.0->langmem) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<2,>=0.6.0->langmem) (0.3.1)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<2,>=0.6.0->langmem) (3.6.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint>=2.0.12->langmem) (1.12.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.8->langmem) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.8->langmem) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.8->langmem) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.8->langmem) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.8->langmem) (0.25.0)\n",
            "Requirement already satisfied: dydantic<1.0.0,>=0.0.8 in /usr/local/lib/python3.12/dist-packages (from trustcall>=0.0.39->langmem) (0.0.8)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.75.0->langchain-anthropic>=0.3.3->langmem) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.75.0->langchain-anthropic>=0.3.3->langmem) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.75.0->langchain-anthropic>=0.3.3->langmem) (0.17.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.75.0->langchain-anthropic>=0.3.3->langmem) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.75.0->langchain-anthropic>=0.3.3->langmem) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.8->langmem) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.8->langmem) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.8->langmem) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.8->langmem) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.3.46->langmem) (3.0.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.3.1->langmem) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3.15->langmem) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3.15->langmem) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3.15->langmem) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.8->langmem) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.8->langmem) (2.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai>=0.3.1->langmem) (2025.11.3)\n"
          ]
        }
      ],
      "source": [
        "pip install langmem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZpmw4y77XQG"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "import os\n",
        "from google.colab import userdata\n",
        "def get_llm_model():\n",
        "  endpoint = userdata.get(\"endpoint\")\n",
        "  model_name = \"gpt-4.1-mini\"\n",
        "  deployment = userdata.get(\"deployment\")\n",
        "  subscription_key = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
        "  api_version = \"2024-12-01-preview\"\n",
        "\n",
        "  llm = AzureChatOpenAI(\n",
        "            azure_endpoint=endpoint,\n",
        "            api_key=subscription_key,\n",
        "            api_version=api_version,\n",
        "            model=deployment,\n",
        "            azure_deployment=deployment,\n",
        "            temperature=0.7,\n",
        "            timeout=300,\n",
        "            max_retries=3,\n",
        "        )\n",
        "  return llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MWAX-Qpczjl"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "embed = AzureOpenAIEmbeddings(\n",
        "    model=\"TxtEmbedAda002\",\n",
        "    # dimensions: Optional[int] = None, # Can specify dimensions with new text-embedding-3 models\n",
        "    azure_endpoint=userdata.get(\"azure_endpoint\"),\n",
        "    api_key=userdata.get(\"api_key\"),\n",
        "    openai_api_version=\"2023-05-15\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dx5RGK0H7qy3",
        "outputId": "7a22addf-8f00-413f-f1b1-30b51978dd70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 31, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_3dcd5944f5', 'id': 'chatcmpl-D0VIqoNLeXfb99xJ8EPONzrgjWkHS', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='lc_run--019be15f-a42d-7d12-8214-45131c6ed0ea-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 31, 'output_tokens': 6, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "# code to test llm model\n",
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
        "    ),\n",
        "    (\"human\", \"I love programming.\"),\n",
        "]\n",
        "ai_msg = get_llm_model().invoke(messages)\n",
        "ai_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkyD5_g2dwd0"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
        "from langchain.agents.middleware import TodoListMiddleware\n",
        "from typing import TypedDict, List, Dict, Any\n",
        "# ---- Shared typed state ----\n",
        "class RCAState(TypedDict):\n",
        "    task: str\n",
        "    output: str\n",
        "    trace: List[Dict[str, Any]]   # structured trace entries\n",
        "    history: List[Any] # Short term memory, working memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_YswsdeQY_5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def extract_json_from_response(response_text: str) -> str:\n",
        "  \"\"\"Extract JSON from markdown-style triple backtick code block with or without newlines.\"\"\"\n",
        "\n",
        "  # Case 1: Code block with '```json\\n ... \\n```'\n",
        "  match = re.search(r'```json\\s*\\n([\\s\\S]*?)\\n```', response_text)\n",
        "  if match:\n",
        "    return match.group(1).strip()\n",
        "\n",
        "  # Case 2: Code block with '```json{...}```' (inline style)\n",
        "  match = re.search(r'```json\\s*([\\s\\S]*?)```', response_text)\n",
        "  if match:\n",
        "    return match.group(1).strip()\n",
        "\n",
        "  # Case 3: Any triple-backtick block (fallback)\n",
        "  match = re.search(r'```\\s*\\n?([\\s\\S]*?)\\n?```', response_text)\n",
        "  if match:\n",
        "    return match.group(1).strip()\n",
        "\n",
        "  # Final fallback: raw content\n",
        "  return response_text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KT10odQQjGy"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def invoke_and_process_response(messages):\n",
        "    json_decoder_prompt = \"\"\"\n",
        "You are an expert in resolving JSON decoding errors.\n",
        "\n",
        "Please review the User input (enclosed in triple backticks) and the AI Output (also enclosed in triple backticks). Your task is to return only the User Expected Output based on your understanding of the users request.\n",
        "\n",
        "We encountered the following error while loading the AI Output into a JSON object: {e}. Kindly provide only the User Expected Output that resolves this issue.\n",
        "\n",
        "User: '''{prompt}'''\n",
        "AI Output: '''{response}'''\n",
        "\n",
        "Lets go step by step.\n",
        "\"\"\"\n",
        "\n",
        "    last_exception = None\n",
        "\n",
        "    for attempt in range(0, 3):\n",
        "        try:\n",
        "            response = get_llm_model().invoke(messages)\n",
        "            content = extract_json_from_response(response.content)\n",
        "\n",
        "            if isinstance(content, str):\n",
        "                content = json.loads(content)\n",
        "\n",
        "            # Handle double-encoded JSON case\n",
        "            if isinstance(content, str):\n",
        "                content = json.loads(content)\n",
        "\n",
        "            return content\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            last_exception = e\n",
        "\n",
        "            # Build recovery system prompt\n",
        "            recovery_prompt = {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": json_decoder_prompt.format(\n",
        "                    e=e,\n",
        "                    prompt=messages,\n",
        "                    response=response.content\n",
        "                )\n",
        "            }\n",
        "\n",
        "            # Replace original message list with the recovery prompt\n",
        "            messages = [recovery_prompt]\n",
        "\n",
        "    print(f\"Model response could not be parsed: {str(last_exception)}\")\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXtQM-PQN4EU"
      },
      "outputs": [],
      "source": [
        "from langchain.agents.middleware import wrap_tool_call\n",
        "from langchain.messages import ToolMessage\n",
        "\n",
        "@wrap_tool_call\n",
        "def handle_tool_errors(request, handler):\n",
        "    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n",
        "    try:\n",
        "        return handler(request)\n",
        "    except Exception as e:\n",
        "        # Return a custom error message to the model\n",
        "        return ToolMessage(\n",
        "            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n",
        "            tool_call_id=request.tool_call[\"id\"]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IesA8XU2OMqO"
      },
      "outputs": [],
      "source": [
        "from langgraph.store.memory import InMemoryStore\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "# Set up storage\n",
        "store = InMemoryStore(\n",
        "    index={\n",
        "        \"dims\": 1536,\n",
        "        \"embed\": embed,\n",
        "    }\n",
        ")\n",
        "\n",
        "checkpointer = InMemorySaver()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IyGSdRNnTvx"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "def append_rca_history(state):\n",
        "    \"\"\"\n",
        "    Append user task, rewritten task, and RCA result to conversation history.\n",
        "    \"\"\"\n",
        "\n",
        "    # Original user task\n",
        "    if state.get(\"task\"):\n",
        "        state[\"history\"].append(\n",
        "            HumanMessage(content=state[\"task\"])\n",
        "        )\n",
        "\n",
        "    # Final RCA response\n",
        "    if state.get(\"output\"):\n",
        "        state[\"history\"].append(\n",
        "            AIMessage(content=state[\"output\"])\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL2yjxFvyvXY"
      },
      "outputs": [],
      "source": [
        "def episodic_recall(query, store, config):\n",
        "    \"\"\"\n",
        "    Retrieve episodic RCA memories relevant to the query.\n",
        "    \"\"\"\n",
        "\n",
        "    namespace = (\"episodic\", config[\"configurable\"][\"user_id\"])\n",
        "\n",
        "    results = store.search(\n",
        "        namespace,\n",
        "        query=query,\n",
        "        limit=1\n",
        "    )\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def procedural_recall(task, store, config):\n",
        "    namespace=(\"procedural\", config[\"configurable\"][\"user_id\"]),\n",
        "    return store.search(\n",
        "        namespace,\n",
        "        query=task,\n",
        "        limit=3\n",
        "    )"
      ],
      "metadata": {
        "id": "7NieTv9AtoZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confidence Decay**\n",
        "What problem does this solve?\n",
        "\n",
        "    Memory gets stale.\n",
        "\n",
        "A rule that worked:\n",
        "\n",
        "    Last month\n",
        "\n",
        "    Under different demand patterns\n",
        "\n",
        "    Before business changes\n",
        "\n",
        "may no longer be correct.\n",
        "\n",
        "**Without decay:**\n",
        "\n",
        "    Old rules dominate\n",
        "\n",
        "    Agent becomes rigid\n",
        "\n",
        "    Wrong confidence persists\n",
        "\n",
        "**With decay:**\n",
        "\n",
        "    Memory slowly loses influence unless reinforced\n",
        "\n",
        "    System stays adaptive\n",
        "\n",
        "**Human analogy**\n",
        "\n",
        "I used to do this, but Im not so sure anymore.\n",
        "\n",
        "Example in our system\n",
        "\n",
        "Semantic memory:\n",
        "\n",
        "\"Promo spikes usually cause stockouts\"\n",
        "Confidence: high\n",
        "\n",
        "\n",
        "After many RCAs with no promo issues:\n",
        "\n",
        "Confidence decays from high  medium  low\n",
        "\n",
        "Eventually ignored or removed\n",
        "\n",
        "**What decay applies to**\n",
        "\n",
        "    Semantic memory\n",
        "\n",
        "    Procedural memory\n",
        "\n",
        "    NOT:\n",
        "\n",
        "    Short-term memory\n",
        "\n",
        "    Raw episodic memory\n",
        "\n",
        "**Why decay matters for agents**\n",
        "\n",
        "    Prevents outdated heuristics\n",
        "\n",
        "    Handles seasonality\n",
        "\n",
        "    Handles business model changes"
      ],
      "metadata": {
        "id": "qgA2X2yMlr3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def semantic_recall(query, store, config, limit: int = 3):\n",
        "\n",
        "    namespace = (\"semantic\", config[\"configurable\"][\"user_id\"])\n",
        "\n",
        "    results = store.search(\n",
        "        namespace,\n",
        "        query=query,\n",
        "        limit=limit * 2  # pull more, filter later\n",
        "    )\n",
        "\n",
        "    now = time.time()\n",
        "    decayed = []\n",
        "\n",
        "    for r in results:\n",
        "        v = r.value\n",
        "        last_used = v.get(\"last_used_at\", now)\n",
        "\n",
        "        age_days = (now - last_used) / 86400\n",
        "\n",
        "        # simple decay rule\n",
        "        if age_days > 30 and v[\"confidence\"] == \"high\":\n",
        "            v[\"confidence\"] = \"medium\"\n",
        "        elif age_days > 60:\n",
        "            v[\"confidence\"] = \"low\"\n",
        "\n",
        "        decayed.append(r)\n",
        "\n",
        "    return decayed[:limit]\n"
      ],
      "metadata": {
        "id": "xbIsp_uvTXJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**history order matters (important)**\n",
        "\n",
        "follow the correct cognitive + agentic order:\n",
        "\n",
        "    Semantic Abstraction memory -> multiple episodic memories  generalized knowledge.\n",
        "\n",
        "    Procedural memory  How should I approach this?\n",
        "\n",
        "    Episodic memory  What happened in similar cases?\n",
        "\n",
        "    Short-term memory  What is happening right now?\n",
        "\n",
        "This is explicitly recommended in:\n",
        "\n",
        "arXiv:2309.02427\n",
        "\n",
        "arXiv:2404.13501\n",
        "\n",
        "LangChain memory-for-agents blog"
      ],
      "metadata": {
        "id": "vBbtDQlaDT6Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwiJ6_v8oEPI"
      },
      "outputs": [],
      "source": [
        "def build_memory_augmented_prompt(\n",
        "    query: str,\n",
        "    state: dict,\n",
        "    config,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Builds a prompt using semantic + procedural + episodic memory + short-term conversation history.\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------------------------\n",
        "    # 0. Semantic Recall (WHY)\n",
        "    # ---------------------------\n",
        "    semantic_memories = semantic_recall(query, store, config, 3)\n",
        "\n",
        "    semantic_context = \"\"\n",
        "    if semantic_memories:\n",
        "        facts = []\n",
        "        for sm in semantic_memories:\n",
        "            v = sm.value\n",
        "            facts.append(\n",
        "                f\"- {v.get('semantic_fact')} \"\n",
        "                f\"(confidence: {v.get('confidence')})\"\n",
        "            )\n",
        "\n",
        "        semantic_context = f\"\"\"\n",
        "    Generalized RCA knowledge:\n",
        "    {chr(10).join(facts)}\n",
        "    \"\"\"\n",
        "    else:\n",
        "        semantic_context = \"No generalized RCA knowledge found.\"\n",
        "\n",
        "\n",
        "    # ---------------------------\n",
        "    # 1. Procedural Recall (HOW)\n",
        "    # ---------------------------\n",
        "    procedural_memories = procedural_recall(query, store, config)\n",
        "\n",
        "    procedural_context = \"\"\n",
        "    if procedural_memories:\n",
        "        procedures = []\n",
        "        for pm in procedural_memories:\n",
        "            proc = pm.value\n",
        "            procedures.append(f\"\"\"\n",
        "- Procedure: {proc.get(\"procedure_name\", \"N/A\")}\n",
        "  Applicable when: {proc.get(\"applicable_when\", \"N/A\")}\n",
        "  Steps:\n",
        "    {chr(10).join([f\"    - {s}\" for s in proc.get(\"steps\", [])])}\n",
        "  Tool heuristics:\n",
        "    {chr(10).join([f\"    - {h}\" for h in proc.get(\"tool_heuristics\", [])])}\n",
        "\"\"\")\n",
        "\n",
        "        procedural_context = f\"\"\"\n",
        "Relevant RCA procedures (how to act):\n",
        "{''.join(procedures)}\n",
        "\"\"\"\n",
        "    else:\n",
        "        procedural_context = \"No relevant RCA procedures found.\"\n",
        "\n",
        "    # ---------------------------\n",
        "    # 2. Episodic Recall\n",
        "    # ---------------------------\n",
        "    episodic_memories = episodic_recall(query, store, config)\n",
        "\n",
        "    episodic_context = \"\"\n",
        "    if episodic_memories:\n",
        "        mem = episodic_memories[0].value  # reflection dict\n",
        "\n",
        "        episodic_context = f\"\"\"\n",
        "Similar past RCA experience:\n",
        "- Current Conversation Match: {mem.get(\"conversation\", \"N/A\")}\n",
        "- Summary: {mem.get(\"conversation_summary\", \"N/A\")}\n",
        "- What worked: {mem.get(\"what_worked\", \"N/A\")}\n",
        "- What to avoid: {mem.get(\"what_to_avoid\", \"N/A\")}\n",
        "\"\"\"\n",
        "    else:\n",
        "        episodic_context = \"No closely related past RCA experience found.\"\n",
        "\n",
        "    # ---------------------------\n",
        "    # 3. Short-term History\n",
        "    # ---------------------------\n",
        "    history = state.get(\"history\", [])\n",
        "\n",
        "    formatted_history = []\n",
        "    for m in history:\n",
        "        if isinstance(m, HumanMessage):\n",
        "            formatted_history.append(f\"USER: {m.content}\")\n",
        "        elif isinstance(m, AIMessage):\n",
        "            formatted_history.append(f\"ASSISTANT: {m.content}\")\n",
        "\n",
        "    history_context = \"\\n\".join(formatted_history) if formatted_history else \"No prior conversation.\"\n",
        "\n",
        "    # ---------------------------\n",
        "    # 4. Final Prompt\n",
        "    # ---------------------------\n",
        "    prompt = f\"\"\"\n",
        "You are an RCA assistant with access to memory.\n",
        "\n",
        "{semantic_context}\n",
        "\n",
        "{procedural_context}\n",
        "\n",
        "{episodic_context}\n",
        "\n",
        "Recent conversation context:\n",
        "{history_context}\n",
        "\n",
        "Instructions:\n",
        "- Follow relevant procedures first\n",
        "- Use past experiences to avoid known pitfalls\n",
        "- Use recent conversation context for continuity\n",
        "\"\"\"\n",
        "\n",
        "    return prompt.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqy4aFl7bA99"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import MessagesState\n",
        "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
        "import json\n",
        "from langchain.agents import create_agent\n",
        "from langchain.tools import tool\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# ReAct agent for hypothesis generation\n",
        "hypothesis_react_agent = create_agent(\n",
        "    model=get_llm_model(),\n",
        "    tools=[\n",
        "        create_manage_memory_tool(namespace=(\"hypothesis\", \"{user_id}\")),\n",
        "        create_search_memory_tool(namespace=(\"hypothesis\", \"{user_id}\"))\n",
        "    ],\n",
        "    middleware=[handle_tool_errors],\n",
        "    store=store,\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "@tool\n",
        "def hypothesis_agent_tool(\n",
        "    task: str,\n",
        "    user_id: str,\n",
        "    query_id: str,\n",
        "    memory_context: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Purpose:\n",
        "        Generate multiple plausible root-cause hypotheses for a given RCA query.\n",
        "\n",
        "    When to use:\n",
        "        Use this tool when an RCA investigation requires enumerating\n",
        "        possible causes of an observed problem. This is typically\n",
        "        the first analytical step after query routing.\n",
        "\n",
        "    Inputs:\n",
        "        - task (str): The resolved and disambiguated user query.\n",
        "        - user_id (str): Identifier of the user or session.\n",
        "        - query_id (str): Unique identifier of the current query/thread.\n",
        "        - memory_context (str): episodic + conversation memory\n",
        "\n",
        "    Output:\n",
        "        - dict: Contains updated fields:\n",
        "            - \"hypotheses\" (List[str]): Newly generated root-cause hypotheses.\n",
        "            - \"trace\" (List[Dict]): Trace entry recording the tool call.\n",
        "\n",
        "    Notes:\n",
        "        - Hypotheses are returned as plain strings with no categorization.\n",
        "        - This tool does not validate hypotheses.\n",
        "        - It may read from long-term memory but only updates the provided data.\n",
        "        - Subsequent tools or agents are expected to validate or eliminate hypotheses.\n",
        "    \"\"\"\n",
        "    # Build messages for LLM\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"\n",
        "You are an RCA hypothesis-generation expert.\n",
        "\n",
        "Context (do not repeat, only use for reasoning):\n",
        "{memory_context}\n",
        "\n",
        "Your task:\n",
        "Given the user input, generate possible root-cause hypotheses.\n",
        "\n",
        "STRICT OUTPUT RULES:\n",
        "1. Output **only valid JSON**.\n",
        "2. Root JSON object must have exactly two fields:\n",
        "   - \"hypotheses\": an array of **plain strings**.\n",
        "   - \"reasoning\": a string explaining how the hypotheses were generated.\n",
        "3. No markdown or code fences.\n",
        "4. No extra commentary or fields.\n",
        "\n",
        "JSON schema:\n",
        "{{\n",
        "  \"hypotheses\": [\"...\", \"...\"],\n",
        "  \"reasoning\": \"...\"\n",
        "}}\n",
        "\"\"\"\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": task}\n",
        "    ]\n",
        "\n",
        "    # Build config dynamically for the agent\n",
        "    config = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": user_id,\n",
        "            \"thread_id\": query_id\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Invoke ReAct agent\n",
        "    result = hypothesis_react_agent.invoke({\"messages\": messages}, config)\n",
        "\n",
        "    # Parse LLM output\n",
        "    final_msg = result[\"messages\"][-1].content\n",
        "    output = process_response(final_msg)\n",
        "\n",
        "    # Extract hypotheses\n",
        "    hypotheses: List[str] = output.get(\"hypotheses\", [])\n",
        "\n",
        "    # Extract internal tool-call reasoning messages\n",
        "    internal_msgs = result[\"messages\"][2:-1]\n",
        "\n",
        "    tool_call_msgs = [\n",
        "        m for m in internal_msgs\n",
        "        if (\n",
        "            (isinstance(m, AIMessage) and getattr(m, \"tool_calls\", None))\n",
        "            or isinstance(m, ToolMessage)\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Create trace entry\n",
        "    trace_entry = {\n",
        "        \"agent\": \"HypothesisAgent\",\n",
        "        \"step\": \"Generated hypotheses\",\n",
        "        \"calls\": serialize_messages(tool_call_msgs),\n",
        "        \"hypotheses\": hypotheses\n",
        "    }\n",
        "\n",
        "    #print(json.dumps(trace_entry, indent=3))\n",
        "\n",
        "    # Return as dict\n",
        "    return {\n",
        "        \"hypotheses\": hypotheses,\n",
        "        \"trace\": [trace_entry]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OAPHZWnmjVH"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def get_daily_sales():\n",
        "    \"\"\"Return daily aggregated sales by store.\"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    df = pd.read_csv(\"/content/sample_data/sales_transactions.csv\", parse_dates=[\"transaction_date\"])\n",
        "\n",
        "    daily = (\n",
        "        df.groupby([\"transaction_date\", \"store_id\", \"store_name\"], as_index=False)[\"quantity_sold\"]\n",
        "        .sum()\n",
        "        .sort_values([\"transaction_date\",\"store_id\"])\n",
        "    )\n",
        "\n",
        "    return daily.to_dict(orient=\"records\")\n",
        "\n",
        "@tool\n",
        "def get_promo_period():\n",
        "    \"\"\"Return promotion start and end date based on sales data.\"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    df = pd.read_csv(\"/content/sample_data/sales_transactions.csv\", parse_dates=[\"transaction_date\"])\n",
        "\n",
        "    # promo flagged in data OR infer from promo tag\n",
        "    promo_df = df[df[\"is_promotion\"] == True]\n",
        "\n",
        "    promo_start = promo_df[\"transaction_date\"].min()\n",
        "    promo_end   = promo_df[\"transaction_date\"].max()\n",
        "\n",
        "    return {\n",
        "        \"promo_start\": str(promo_start.date()),\n",
        "        \"promo_end\": str(promo_end.date())\n",
        "    }\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_promo_sales_by_store():\n",
        "    \"\"\"Return total promotion-period sales by store.\"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    df = pd.read_csv(\"/content/sample_data/sales_transactions.csv\", parse_dates=[\"transaction_date\"])\n",
        "\n",
        "    # identify promo rows\n",
        "    promo_df = df[df[\"is_promotion\"] == True]\n",
        "\n",
        "    promo_sales = (\n",
        "        promo_df.groupby([\"store_id\",\"store_name\"], as_index=False)[\"quantity_sold\"]\n",
        "        .sum()\n",
        "        .rename(columns={\"quantity_sold\": \"promo_qty_sold\"})\n",
        "    )\n",
        "\n",
        "    return promo_sales.to_dict(orient=\"records\")\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_sales_data():\n",
        "    \"\"\"Return sales data as list of dicts.\"\"\"\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(\"/content/sample_data/sales_transactions.csv\", parse_dates=[\"transaction_date\"])\n",
        "    return df.to_dict(orient=\"records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTc_E2zp9y8v"
      },
      "outputs": [],
      "source": [
        "def serialize_messages(msgs):\n",
        "    \"\"\"Convert LangChain messages/tool-calls to JSON-safe dicts.\"\"\"\n",
        "    cleaned = []\n",
        "\n",
        "    for m in msgs:\n",
        "        entry = {\n",
        "            \"type\": m.__class__.__name__,\n",
        "            \"content\": m.content\n",
        "        }\n",
        "        # If the message contains tool calls\n",
        "        if hasattr(m, \"tool_calls\") and m.tool_calls:\n",
        "            entry[\"tool_calls\"] = [\n",
        "                {\n",
        "                    \"name\": tc.get(\"name\"),\n",
        "                    \"args\": tc.get(\"args\"),\n",
        "                    \"id\": tc.get(\"id\")\n",
        "                }\n",
        "                for tc in m.tool_calls\n",
        "            ]\n",
        "\n",
        "        # If message is a ToolMessage\n",
        "        if hasattr(m, \"tool_call_id\"):\n",
        "            entry[\"tool_call_id\"] = m.tool_call_id\n",
        "\n",
        "        cleaned.append(entry)\n",
        "\n",
        "    return cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6pnNNdr2OiT"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def process_response(response_content):\n",
        "    json_decoder_prompt = \"\"\"\n",
        "You are an expert in resolving JSON decoding errors.\n",
        "\n",
        "Please review the AI Output (enclosed in triple backticks).\n",
        "\n",
        "We encountered the following error while loading the AI Output into a JSON object: {e}. Kindly resolve this issue.\n",
        "\n",
        "AI Output: '''{response}'''\n",
        "\n",
        "Return ONLY the corrected JSON.\n",
        "\"\"\"\n",
        "\n",
        "    last_exception = None\n",
        "\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            # Try to extract/parse the JSON the normal way\n",
        "            content = extract_json_from_response(response_content)\n",
        "\n",
        "            if isinstance(content, str):\n",
        "                content = json.loads(content)\n",
        "\n",
        "            # Handle double-encoded JSON\n",
        "            if isinstance(content, str):\n",
        "                content = json.loads(content)\n",
        "\n",
        "            return content\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            last_exception = e\n",
        "\n",
        "            # Build recovery prompt asking LLM to fix the JSON\n",
        "            recovery_prompt = {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": json_decoder_prompt.format(\n",
        "                    e=str(e),\n",
        "                    response=response_content\n",
        "                )\n",
        "            }\n",
        "\n",
        "            # Re-invoke the model with the recovery request\n",
        "            fixed_response = get_llm_model().invoke([recovery_prompt])\n",
        "\n",
        "            # Replace response_content with model's corrected attempt\n",
        "            response_content = fixed_response.content\n",
        "\n",
        "    # If all attempts failed\n",
        "    print(f\"Model response could not be parsed: {str(last_exception)}\")\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scQ8UzebmVkD"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain.tools import tool\n",
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "from typing import Dict, Any, List\n",
        "import json\n",
        "\n",
        "sales_tools = [\n",
        "    get_daily_sales,\n",
        "    get_promo_period,\n",
        "    get_promo_sales_by_store,\n",
        "    get_sales_data,\n",
        "    create_manage_memory_tool(namespace=(\"sales\", \"{user_id}\")),\n",
        "    create_search_memory_tool(namespace=(\"sales\", \"{user_id}\"))\n",
        "]\n",
        "\n",
        "sales_react_agent = create_agent(\n",
        "    model=get_llm_model(),\n",
        "    tools=sales_tools,\n",
        "    middleware=[handle_tool_errors],\n",
        "    store=store,\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "\n",
        "@tool\n",
        "def sales_analysis_agent_tool(\n",
        "    task: str,\n",
        "    hypotheses: List[str],\n",
        "    user_id: str,\n",
        "    query_id: str,\n",
        "    memory_context: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Purpose:\n",
        "        Analyze sales and promotion data to evaluate hypotheses that may\n",
        "        explain observed issues in an RCA investigation.\n",
        "\n",
        "    When to use:\n",
        "        Use this tool after hypotheses have been generated and when\n",
        "        sales, demand, forecasting, or promotion-related factors may\n",
        "        contribute to the problem.\n",
        "\n",
        "    Inputs:\n",
        "        - task (str):\n",
        "            The resolved RCA task or problem statement to analyze.\n",
        "        - hypotheses (List[str]):\n",
        "            A list of candidate root-cause hypotheses to be validated\n",
        "            from a sales perspective.\n",
        "        - user_id (str):\n",
        "            Identifier for the user or session, used for scoped memory access.\n",
        "        - query_id (str):\n",
        "            Unique identifier for the current RCA query or thread.\n",
        "        - memory_context (str): episodic + conversation memory\n",
        "\n",
        "    Output:\n",
        "        - dict:\n",
        "            Contains the following fields:\n",
        "            - \"sales_insights\":\n",
        "                Structured findings derived from sales and promotion data\n",
        "                that support or refute the provided hypotheses.\n",
        "            - \"trace\":\n",
        "                A list of trace entries capturing tool calls and reasoning\n",
        "                steps performed during the analysis.\n",
        "    Notes:\n",
        "        - This tool may call sales and promotion data tools as needed.\n",
        "        - The output is strictly structured and intended for downstream\n",
        "          RCA agents or summarization steps.\n",
        "        - The tool does not mutate external state.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1  Filter sales-related hypotheses\n",
        "    sales_related_hypotheses = [\n",
        "        h for h in hypotheses\n",
        "        if any(\n",
        "            k in h.lower()\n",
        "            for k in [\n",
        "                \"sales\",\n",
        "                \"demand\",\n",
        "                \"promotion\",\n",
        "                \"spike\",\n",
        "                \"forecast\",\n",
        "                \"underestimated\"\n",
        "            ]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Fallback: analyze all hypotheses if none are sales-specific\n",
        "    if not sales_related_hypotheses:\n",
        "        sales_related_hypotheses = hypotheses\n",
        "\n",
        "    # Step 2  Build LLM messages\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"\n",
        "You are a Sales Analysis Agent for RCA.\n",
        "\n",
        "Context (do not repeat, only use for reasoning):\n",
        "{memory_context}\n",
        "\n",
        "Your responsibilities:\n",
        "- Use available tools to analyze sales patterns\n",
        "- Validate or refute sales-related hypotheses\n",
        "\n",
        "STRICT OUTPUT RULES:\n",
        "1. Output ONLY valid JSON\n",
        "2. Root JSON object MUST contain EXACTLY ONE key: \"sales_insights\"\n",
        "3. NO extra keys, commentary, or markdown\n",
        "\n",
        "JSON schema:\n",
        "{{\n",
        "  \"sales_insights\": {...}\n",
        "}}\n",
        "\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "Task: {task}\n",
        "Hypotheses: {sales_related_hypotheses}\n",
        "\"\"\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Step 3  Build config (same pattern as hypothesis_agent)\n",
        "    config = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": user_id,\n",
        "            \"thread_id\": query_id\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Step 4  Invoke ReAct agent\n",
        "    result = sales_react_agent.invoke({\"messages\": messages}, config)\n",
        "\n",
        "    final_msg = result[\"messages\"][-1].content\n",
        "    output = process_response(final_msg)\n",
        "\n",
        "    sales_insights = output.get(\"sales_insights\")\n",
        "\n",
        "    # Step 5  Extract internal tool-call trace\n",
        "    # Skip system(0), user(1), final(-1)\n",
        "    internal_msgs = result[\"messages\"][2:-1]\n",
        "\n",
        "    tool_call_msgs = [\n",
        "        m for m in internal_msgs\n",
        "        if (\n",
        "            (isinstance(m, AIMessage) and getattr(m, \"tool_calls\", None))\n",
        "            or isinstance(m, ToolMessage)\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    trace_entry = {\n",
        "        \"agent\": \"SalesAnalysisAgent\",\n",
        "        \"step\": \"Validated sales hypotheses\",\n",
        "        \"calls\": serialize_messages(tool_call_msgs),\n",
        "        \"sales_insights\": sales_insights\n",
        "    }\n",
        "\n",
        "    #print(json.dumps(trace_entry, indent=3))\n",
        "\n",
        "    # Step 6  Return\n",
        "    return {\n",
        "        \"sales_insights\": sales_insights,\n",
        "        \"trace\": [trace_entry]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Glq2J4nvwNUS"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def get_unique_stores() -> dict:\n",
        "    \"\"\"Return list of unique store_ids from sales and inventory timeline.\"\"\"\n",
        "    import pandas as pd\n",
        "    sales_df = pd.read_csv(\"/content/sample_data/sales_transactions.csv\", parse_dates=[\"transaction_date\"])\n",
        "    inv_df   = pd.read_csv(\"/content/sample_data/inventory_transactions.csv\", parse_dates=[\"transaction_date\"])\n",
        "\n",
        "    stores = sorted(pd.concat([\n",
        "        sales_df[\"store_id\"],\n",
        "        inv_df[\"store_id\"]\n",
        "    ]).dropna().unique())\n",
        "\n",
        "    return {\"stores\": stores}\n",
        "\n",
        "def drop_store_name(df):\n",
        "    return df.drop(columns=[\"store_name\"], errors=\"ignore\")\n",
        "\n",
        "@tool\n",
        "def theoretical_onhand_after_promo_sales(promo_start: str, promo_end: str):\n",
        "    \"\"\"\n",
        "    Compute theoretical on-hand inventory after promo sales for each store.\n",
        "    Inputs: promo_start (YYYY-MM-DD), promo_end (YYYY-MM-DD)\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    # Load data\n",
        "    sales = pd.read_csv(\"/content/sample_data/sales_transactions.csv\", parse_dates=[\"transaction_date\"])\n",
        "    inv = pd.read_csv(\"/content/sample_data/inventory_transactions.csv\", parse_dates=[\"transaction_date\"])\n",
        "\n",
        "    promo_start = pd.to_datetime(promo_start)\n",
        "    promo_end = pd.to_datetime(promo_end)\n",
        "\n",
        "    # === 1. Compute as_of date (day before promo_start) ===\n",
        "    as_of = promo_start - pd.Timedelta(days=1)\n",
        "\n",
        "    # === 2. START INVENTORY (sum of RECEIPT qty before promo) ===\n",
        "    inv_receipts_before = inv[\n",
        "        (inv[\"transaction_date\"] <= as_of) &\n",
        "        (inv[\"transaction_type\"] == \"RECEIPT\")\n",
        "    ]\n",
        "\n",
        "    start_inv = (\n",
        "        inv_receipts_before.groupby(\"destination_location\", as_index=False)[\"quantity\"]\n",
        "        .sum()\n",
        "        .rename(columns={\"destination_location\": \"store_id\", \"quantity\": \"start_receipt_qty\"})\n",
        "    )\n",
        "\n",
        "\n",
        "    # === 3. NET INVENTORY CHANGES AFTER as_of DATE ===\n",
        "    inv_changes = inv[inv[\"transaction_date\"] > as_of].copy()\n",
        "\n",
        "    # Convert NONE  NaN\n",
        "    inv_changes[\"destination_location\"] = inv_changes[\"destination_location\"].replace(\"NONE\", pd.NA)\n",
        "\n",
        "    # Map to store\n",
        "    inv_changes[\"target_store\"] = inv_changes[\"destination_location\"].fillna(inv_changes[\"store_id\"])\n",
        "\n",
        "    inv_net_after = (\n",
        "        inv_changes.groupby(\"target_store\", as_index=False)[\"quantity\"]\n",
        "        .sum()\n",
        "        .rename(columns={\"target_store\": \"store_id\", \"quantity\": \"net_qty_after\"})\n",
        "    )\n",
        "\n",
        "    # === 4. PROMO REPLENISHMENT ===\n",
        "    # (Receipts on the promo_start date)\n",
        "    promo_repl = inv[\n",
        "        (inv[\"transaction_date\"] == promo_start) &\n",
        "        (inv[\"transaction_type\"] == \"RECEIPT\")\n",
        "    ]\n",
        "\n",
        "    promo_repl_by_store = (\n",
        "        promo_repl.groupby(\"destination_location\", as_index=False)[\"quantity\"]\n",
        "        .sum()\n",
        "        .rename(columns={\"destination_location\": \"store_id\", \"quantity\": \"promo_repl_qty\"})\n",
        "    )\n",
        "\n",
        "    # === 5. PROMO SALES ===\n",
        "    promo_sales = sales[\n",
        "        (sales[\"transaction_date\"] >= promo_start) &\n",
        "        (sales[\"transaction_date\"] <= promo_end)\n",
        "    ]\n",
        "\n",
        "    promo_by_store = (\n",
        "        promo_sales.groupby([\"store_id\",\"store_name\"], as_index=False)[\"quantity_sold\"]\n",
        "        .sum()\n",
        "        .rename(columns={\"quantity_sold\": \"promo_qty_sold\"})\n",
        "    )\n",
        "\n",
        "    # === 6. MERGE EVERYTHING ===\n",
        "    stores = sales[[\"store_id\",\"store_name\"]].drop_duplicates()\n",
        "\n",
        "    summary = (\n",
        "        stores\n",
        "        .merge(drop_store_name(start_inv), on=\"store_id\", how=\"left\")\n",
        "        .merge(drop_store_name(inv_net_after), on=\"store_id\", how=\"left\")\n",
        "        .merge(drop_store_name(promo_by_store), on=\"store_id\", how=\"left\")\n",
        "        .merge(drop_store_name(promo_repl_by_store), on=\"store_id\", how=\"left\")\n",
        "    ).fillna(0)\n",
        "\n",
        "    # === 7. COMPUTE THEORETICAL INVENTORY ===\n",
        "    summary[\"theoretical_after_changes\"] = (\n",
        "        summary[\"start_receipt_qty\"] + summary[\"net_qty_after\"]\n",
        "    )\n",
        "\n",
        "    summary[\"theoretical_onhand_after_promo_sales\"] = (\n",
        "        summary[\"theoretical_after_changes\"] - summary[\"promo_qty_sold\"]\n",
        "    )\n",
        "\n",
        "    return summary.to_dict(orient=\"records\")\n",
        "\n",
        "@tool\n",
        "def get_daily_inventory_for_store(store_id: str):\n",
        "    \"\"\"\n",
        "    Return daily inventory on-hand timeline for a given store_id.\n",
        "    Computes:\n",
        "      - daily inventory movements (receipts, adjustments, transfers)\n",
        "      - daily sales\n",
        "      - net change\n",
        "      - running inventory\n",
        "\n",
        "    Args:\n",
        "      store_id: store code, e.g., \"S001\"\n",
        "\n",
        "    Returns:\n",
        "      List of dicts with:\n",
        "      transaction_date, quantity, quantity_sold,\n",
        "      net_change, running_inventory\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    # Load datasets\n",
        "    inv = pd.read_csv(\"/content/sample_data/inventory_transactions.csv\",\n",
        "                      parse_dates=[\"transaction_date\"])\n",
        "    sales = pd.read_csv(\"/content/sample_data/sales_transactions.csv\",\n",
        "                        parse_dates=[\"transaction_date\"])\n",
        "\n",
        "    # ---------- CLEANING ----------\n",
        "    # Convert \"NONE\"  NaN\n",
        "    inv[\"destination_location\"] = inv[\"destination_location\"].replace(\"NONE\", pd.NA)\n",
        "\n",
        "    # Always use store_id (never destination_location)\n",
        "    inv[\"store\"] = inv[\"store_id\"]\n",
        "\n",
        "    # ---------- INVENTORY MOVEMENTS ----------\n",
        "    daily_inv_moves = (\n",
        "        inv.groupby([\"transaction_date\", \"store\"], as_index=False)[\"quantity\"]\n",
        "        .sum()\n",
        "    )\n",
        "\n",
        "    # ---------- SALES ----------\n",
        "    daily_sales = (\n",
        "        sales.groupby([\"transaction_date\", \"store_id\"], as_index=False)[\"quantity_sold\"]\n",
        "        .sum()\n",
        "        .rename(columns={\"store_id\": \"store\"})\n",
        "    )\n",
        "\n",
        "    # ---------- MERGE TIMELINE ----------\n",
        "    timeline = pd.merge(\n",
        "        daily_inv_moves, daily_sales, on=[\"transaction_date\", \"store\"], how=\"outer\"\n",
        "    ).fillna(0)\n",
        "\n",
        "    # Compute net change\n",
        "    timeline[\"net_change\"] = timeline[\"quantity\"] - timeline[\"quantity_sold\"]\n",
        "\n",
        "    # Sort\n",
        "    timeline = timeline.sort_values([\"store\", \"transaction_date\"])\n",
        "\n",
        "    # Running inventory\n",
        "    timeline[\"running_inventory\"] = (\n",
        "        timeline.groupby(\"store\")[\"net_change\"].cumsum()\n",
        "    )\n",
        "\n",
        "    # ---------- FILTER FOR SPECIFIC STORE ----------\n",
        "    result = timeline[timeline[\"store\"] == store_id]\n",
        "\n",
        "    return result.to_dict(orient=\"records\")\n",
        "\n",
        "@tool\n",
        "def get_adjustments():\n",
        "    \"\"\"Return all shrinkage/adjustment rows.\"\"\"\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(\"/content/sample_data/inventory_transactions.csv\",\n",
        "                     parse_dates=[\"transaction_date\"])\n",
        "    adjustments = df[df[\"transaction_type\"] == \"ADJUSTMENT\"]\n",
        "    return adjustments.to_dict(orient=\"records\")\n",
        "\n",
        "@tool\n",
        "def get_shrinkage_before_promo(promo_start: str):\n",
        "    \"\"\"\n",
        "    promo_start: promo start date as 'YYYY-MM-DD'\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(\"/content/sample_data/inventory_transactions.csv\",\n",
        "                     parse_dates=[\"transaction_date\"])\n",
        "    promo_start = pd.to_datetime(promo_start)\n",
        "\n",
        "    result = df[\n",
        "        (df[\"transaction_type\"] == \"ADJUSTMENT\") &\n",
        "        (df[\"transaction_date\"] < promo_start)\n",
        "    ]\n",
        "    return result.to_dict(orient=\"records\")\n",
        "\n",
        "@tool\n",
        "def get_shrinkage_during_promo(promo_start: str, promo_end: str):\n",
        "    \"\"\"get shrinkage during promo\"\"\"\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(\"/content/sample_data/inventory_transactions.csv\",\n",
        "                     parse_dates=[\"transaction_date\"])\n",
        "\n",
        "    promo_start = pd.to_datetime(promo_start)\n",
        "    promo_end   = pd.to_datetime(promo_end)\n",
        "\n",
        "    result = df[\n",
        "        (df[\"transaction_type\"] == \"ADJUSTMENT\") &\n",
        "        (df[\"transaction_date\"] >= promo_start) &\n",
        "        (df[\"transaction_date\"] <= promo_end)\n",
        "    ]\n",
        "    return result.to_dict(orient=\"records\")\n",
        "\n",
        "@tool\n",
        "def get_delayed_replenishments():\n",
        "    \"\"\"Return all inventory rows with DELAYED note.\"\"\"\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(\"/content/sample_data/inventory_transactions.csv\",\n",
        "                     parse_dates=[\"transaction_date\"])\n",
        "\n",
        "    delayed = df[df[\"notes\"].str.contains(\"DELAYED\", na=False)]\n",
        "    return delayed.to_dict(orient=\"records\")\n",
        "\n",
        "@tool\n",
        "def get_promo_replenishment_for_date(date: str):\n",
        "    \"\"\"\n",
        "    date: day replenishment was expected (promo_start or a different date)\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(\"/content/sample_data/inventory_transactions.csv\",\n",
        "                     parse_dates=[\"transaction_date\"])\n",
        "    date = pd.to_datetime(date)\n",
        "\n",
        "    promo_repl = df[\n",
        "        (df[\"transaction_date\"] == date) &\n",
        "        (df[\"transaction_type\"] == \"RECEIPT\")\n",
        "    ]\n",
        "    return promo_repl.to_dict(orient=\"records\")\n",
        "\n",
        "@tool\n",
        "def get_all_transfers():\n",
        "    \"\"\"Return all transfer rows.\"\"\"\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(\"/content/sample_data/inventory_transactions.csv\",\n",
        "                     parse_dates=[\"transaction_date\"])\n",
        "\n",
        "    transfers = df[df[\"transaction_type\"] == \"TRANSFER\"]\n",
        "    return transfers.to_dict(orient=\"records\")\n",
        "\n",
        "@tool\n",
        "def get_transfers_for_date(date: str):\n",
        "    \"\"\"get transfers for a given date\"\"\"\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(\"/content/sample_data/inventory_transactions.csv\",\n",
        "                     parse_dates=[\"transaction_date\"])\n",
        "\n",
        "    date = pd.to_datetime(date)\n",
        "    result = df[\n",
        "        (df[\"transaction_type\"] == \"TRANSFER\") &\n",
        "        (df[\"transaction_date\"] == date)\n",
        "    ]\n",
        "    return result.to_dict(orient=\"records\")\n",
        "\n",
        "@tool\n",
        "def get_emergency_receipts():\n",
        "    \"\"\"get emergency receipts\"\"\"\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(\"/content/sample_data/inventory_transactions.csv\",\n",
        "                     parse_dates=[\"transaction_date\"])\n",
        "\n",
        "    emergency = df[df[\"notes\"].str.contains(\"Emergency\", na=False)]\n",
        "    return emergency.to_dict(orient=\"records\")\n",
        "\n",
        "@tool\n",
        "def get_inventory_data():\n",
        "    \"\"\"Return inventory movements as list of dicts.\"\"\"\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(\"/content/sample_data/inventory_transactions.csv\", parse_dates=[\"transaction_date\"])\n",
        "    return df.to_dict(orient=\"records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl-hOfFJbpSv"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain.tools import tool\n",
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "from typing import Dict, Any, List\n",
        "import json\n",
        "\n",
        "inventory_tools = [\n",
        "    get_promo_period,\n",
        "    get_unique_stores,\n",
        "    theoretical_onhand_after_promo_sales,\n",
        "    get_daily_inventory_for_store,\n",
        "    get_adjustments,\n",
        "    get_shrinkage_before_promo,\n",
        "    get_shrinkage_during_promo,\n",
        "    get_delayed_replenishments,\n",
        "    get_promo_replenishment_for_date,\n",
        "    get_all_transfers,\n",
        "    get_transfers_for_date,\n",
        "    get_emergency_receipts,\n",
        "    get_inventory_data,\n",
        "    create_manage_memory_tool(namespace=(\"inventory\", \"{user_id}\")),\n",
        "    create_search_memory_tool(namespace=(\"inventory\", \"{user_id}\"))\n",
        "]\n",
        "\n",
        "inventory_react_agent = create_agent(\n",
        "    model=get_llm_model(),\n",
        "    tools=inventory_tools,\n",
        "    middleware=[handle_tool_errors],\n",
        "    store=store,\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "\n",
        "@tool\n",
        "def inventory_analysis_agent_tool(\n",
        "    task: str,\n",
        "    hypotheses: List[str],\n",
        "    user_id: str,\n",
        "    query_id: str,\n",
        "    memory_context: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Purpose:\n",
        "        Analyze inventory movements, replenishments, transfers, and\n",
        "        adjustments to validate inventory-related RCA hypotheses.\n",
        "\n",
        "    When to use:\n",
        "        Use this tool when stock availability, shrinkage, replenishment\n",
        "        timing, transfers, or warehouse operations may contribute to\n",
        "        the observed problem.\n",
        "\n",
        "    Inputs:\n",
        "        - task (str): Resolved RCA task or problem statement\n",
        "        - hypotheses (List[str]): Candidate hypotheses to validate\n",
        "        - user_id (str): User/session identifier for scoped memory access\n",
        "        - query_id (str): Query/thread identifier\n",
        "        - memory_context (str): episodic + conversation memory\n",
        "\n",
        "    Output:\n",
        "        - dict:\n",
        "            - \"inventory_insights\": Structured inventory analysis\n",
        "            - \"trace\": Tool-call trace for observability\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1  Filter inventory-related hypotheses\n",
        "    inventory_related_hypotheses = [\n",
        "        h for h in hypotheses\n",
        "        if any(\n",
        "            k in h.lower()\n",
        "            for k in [\n",
        "                \"inventory\",\n",
        "                \"stock\",\n",
        "                \"supply\",\n",
        "                \"replenish\",\n",
        "                \"transfer\",\n",
        "                \"shrink\",\n",
        "                \"adjust\",\n",
        "                \"warehouse\"\n",
        "            ]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Fallback: analyze all hypotheses if none are inventory-specific\n",
        "    if not inventory_related_hypotheses:\n",
        "        inventory_related_hypotheses = hypotheses\n",
        "\n",
        "    # Step 2  Build LLM messages\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"\n",
        "You are the Inventory RCA Agent.\n",
        "\n",
        "Context (do not repeat, only use for reasoning):\n",
        "{memory_context}\n",
        "\n",
        "Your responsibilities:\n",
        "- Analyze inventory levels, movements, transfers, adjustments, and replenishments\n",
        "- Use available tools via a ReAct loop\n",
        "- Produce structured insights\n",
        "\n",
        "STRICT OUTPUT RULES:\n",
        "1. Output ONLY valid JSON\n",
        "2. Root JSON object MUST contain EXACTLY ONE key: \"inventory_insights\"\n",
        "3. NO extra keys, markdown, or commentary\n",
        "\n",
        "JSON schema:\n",
        "{{\n",
        "  \"inventory_insights\": {{...}}\n",
        "}}\n",
        "\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "Task: {task}\n",
        "Hypotheses to validate: {inventory_related_hypotheses}\n",
        "\"\"\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Step 3  Build config\n",
        "    config = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": user_id,\n",
        "            \"thread_id\": query_id\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Step 4  Invoke ReAct agent\n",
        "    result = inventory_react_agent.invoke({\"messages\": messages}, config)\n",
        "\n",
        "    final_msg = result[\"messages\"][-1].content\n",
        "    output = process_response(final_msg)\n",
        "\n",
        "    inventory_insights = output.get(\"inventory_insights\")\n",
        "\n",
        "    # Step 5  Extract internal tool-call trace\n",
        "    # Skip system(0), user(1), final(-1)\n",
        "    internal_msgs = result[\"messages\"][2:-1]\n",
        "\n",
        "    tool_call_msgs = [\n",
        "        m for m in internal_msgs\n",
        "        if (\n",
        "            (isinstance(m, AIMessage) and getattr(m, \"tool_calls\", None))\n",
        "            or isinstance(m, ToolMessage)\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    trace_entry = {\n",
        "        \"agent\": \"InventoryAnalysisAgent\",\n",
        "        \"step\": \"Validated inventory hypotheses\",\n",
        "        \"calls\": serialize_messages(tool_call_msgs),\n",
        "        \"inventory_insights\": inventory_insights\n",
        "    }\n",
        "\n",
        "    #print(json.dumps(trace_entry, indent=3))\n",
        "\n",
        "    # Step 6  Return result (no state mutation)\n",
        "    return {\n",
        "        \"inventory_insights\": inventory_insights,\n",
        "        \"trace\": [trace_entry]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z8YQKSHbzj4"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain.tools import tool\n",
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "from typing import Dict, Any, List\n",
        "import json\n",
        "\n",
        "validation_react_agent = create_agent(\n",
        "    model=get_llm_model(),\n",
        "    tools=[\n",
        "        create_manage_memory_tool(namespace=(\"hypothesis_validation\", \"{user_id}\")),\n",
        "        create_search_memory_tool(namespace=(\"hypothesis_validation\", \"{user_id}\"))\n",
        "    ],\n",
        "    middleware=[handle_tool_errors],\n",
        "    store=store,\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "\n",
        "@tool\n",
        "def hypothesis_validation_agent_tool(\n",
        "    hypotheses: List[str],\n",
        "    sales_insights: Dict[str, Any],\n",
        "    inventory_insights: Dict[str, Any],\n",
        "    user_id: str,\n",
        "    query_id: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Purpose:\n",
        "        Validate each hypothesis by cross-referencing sales and inventory\n",
        "        insights gathered during the RCA investigation.\n",
        "\n",
        "    When to use:\n",
        "        Use this tool after domain-specific analysis tools (e.g., Sales,\n",
        "        Inventory) have produced structured insights.\n",
        "\n",
        "    Inputs:\n",
        "        - hypotheses (List[str]):\n",
        "            Hypotheses to be validated.\n",
        "        - sales_insights (dict):\n",
        "            Output from the Sales Analysis tool.\n",
        "        - inventory_insights (dict):\n",
        "            Output from the Inventory Analysis tool.\n",
        "        - user_id (str):\n",
        "            User/session identifier for scoped memory access.\n",
        "        - query_id (str):\n",
        "            Query/thread identifier.\n",
        "\n",
        "    Output:\n",
        "        - dict:\n",
        "            - \"validated\": Mapping of hypothesis  true / false\n",
        "            - \"reasoning\": Mapping of hypothesis  explanation\n",
        "            - \"trace\": Tool-call trace for observability\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1  Build LLM messages\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"\n",
        "Validate each hypothesis using sales and inventory insights.\n",
        "\n",
        "STRICT OUTPUT RULES:\n",
        "1. Output ONLY valid JSON\n",
        "2. No markdown or code fences\n",
        "3. No extra fields or commentary\n",
        "\n",
        "JSON schema:\n",
        "{\n",
        "  \"validated\": { \"hypothesis\": true | false },\n",
        "  \"reasoning\": { \"hypothesis\": \"explanation\" }\n",
        "}\n",
        "\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "Hypotheses:\n",
        "{hypotheses}\n",
        "\n",
        "Sales insights:\n",
        "{sales_insights}\n",
        "\n",
        "Inventory insights:\n",
        "{inventory_insights}\n",
        "\"\"\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Step 2  Build config\n",
        "    config = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": user_id,\n",
        "            \"thread_id\": query_id\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Step 3  Invoke ReAct agent\n",
        "    result = validation_react_agent.invoke({\"messages\": messages}, config)\n",
        "\n",
        "    final_msg = result[\"messages\"][-1].content\n",
        "    resp = process_response(final_msg)\n",
        "\n",
        "    # Step 4  Extract internal tool-call trace\n",
        "    # Skip system(0), user(1), final(-1)\n",
        "    internal_msgs = result[\"messages\"][2:-1]\n",
        "\n",
        "    tool_call_msgs = [\n",
        "        m for m in internal_msgs\n",
        "        if (\n",
        "            (isinstance(m, AIMessage) and getattr(m, \"tool_calls\", None))\n",
        "            or isinstance(m, ToolMessage)\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    trace_entry = {\n",
        "        \"agent\": \"HypothesisValidationAgent\",\n",
        "        \"step\": \"Validated hypotheses\",\n",
        "        \"calls\": serialize_messages(tool_call_msgs),\n",
        "        \"details\": resp\n",
        "    }\n",
        "\n",
        "    #print(json.dumps(trace_entry, indent=3))\n",
        "\n",
        "    # Step 5  Return result (no state mutation)\n",
        "    return {\n",
        "        \"validated\": resp.get(\"validated\"),\n",
        "        \"reasoning\": resp.get(\"reasoning\"),\n",
        "        \"trace\": [trace_entry]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjUi33AKb2r9"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain.tools import tool\n",
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "from typing import Dict, Any, List\n",
        "import json\n",
        "\n",
        "\n",
        "root_cause_react_agent = create_agent(\n",
        "    model=get_llm_model(),\n",
        "    tools=[\n",
        "    ],\n",
        "    middleware=[handle_tool_errors],\n",
        "    store=store,\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "\n",
        "@tool\n",
        "def root_cause_analysis_agent_tool(\n",
        "    validated_hypotheses: Dict[str, bool],\n",
        "    sales_insights: Dict[str, Any],\n",
        "    inventory_insights: Dict[str, Any],\n",
        "    trace: List[Dict[str, Any]],\n",
        "    user_id: str,\n",
        "    query_id: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Purpose:\n",
        "        Produce the final Root Cause Analysis by synthesizing validated\n",
        "        hypotheses, sales insights, inventory insights, and prior analysis\n",
        "        trace into a structured RCA outcome.\n",
        "\n",
        "    When to use:\n",
        "        Use this tool after hypothesis validation has been completed.\n",
        "\n",
        "    Inputs:\n",
        "        - validated_hypotheses (dict): Hypothesis  true/false mapping\n",
        "        - sales_insights (dict): Sales analysis output\n",
        "        - inventory_insights (dict): Inventory analysis output\n",
        "        - trace (list): Prior agent trace entries\n",
        "        - user_id (str): User/session identifier for scoped memory access\n",
        "        - query_id (str): Query/thread identifier\n",
        "\n",
        "    Output:\n",
        "        - dict:\n",
        "            - \"root_cause\": Final structured RCA\n",
        "            - \"reasoning\": Explanation of RCA decisions\n",
        "            - \"trace\": Tool-call trace for observability\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------- Step 1: Generate structured Root Cause JSON ----------\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"\n",
        "Produce a final Root Cause Analysis.\n",
        "\n",
        "Include:\n",
        "- primary root causes\n",
        "- supporting evidence\n",
        "- contributing factors\n",
        "- timeline\n",
        "- recommendations\n",
        "\n",
        "STRICT OUTPUT RULES:\n",
        "1. Output ONLY valid JSON\n",
        "2. No markdown or code fences\n",
        "3. No extra commentary\n",
        "4. JSON MUST contain EXACTLY two top-level keys:\n",
        "   - \"root_cause\"\n",
        "   - \"reasoning\"\n",
        "\n",
        "JSON schema:\n",
        "{\n",
        "  \"root_cause\": {\n",
        "    \"primary_root_causes\": [\"string\"],\n",
        "    \"supporting_evidence\": {\n",
        "      \"sales\": {},\n",
        "      \"inventory\": {},\n",
        "      \"cross_analysis\": {}\n",
        "    },\n",
        "    \"contributing_factors\": [\"string\"],\n",
        "    \"timeline\": [\n",
        "      { \"date\": \"YYYY-MM-DD\", \"event\": \"string\" }\n",
        "    ],\n",
        "    \"recommendations\": [\"string\"]\n",
        "  },\n",
        "  \"reasoning\": {\n",
        "    \"primary_root_causes\": \"explanation\",\n",
        "    \"contributing_factors\": \"explanation\",\n",
        "    \"supporting_evidence\": \"explanation\",\n",
        "    \"timeline\": \"explanation\",\n",
        "    \"recommendations\": \"explanation\"\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "Validated hypotheses:\n",
        "{validated_hypotheses}\n",
        "\n",
        "Sales insights:\n",
        "{sales_insights}\n",
        "\n",
        "Inventory insights:\n",
        "{inventory_insights}\n",
        "\n",
        "Prior trace:\n",
        "{trace}\n",
        "\"\"\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    config = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": user_id,\n",
        "            \"thread_id\": query_id\n",
        "        }\n",
        "    }\n",
        "\n",
        "    result = root_cause_react_agent.invoke({\"messages\": messages}, config)\n",
        "\n",
        "    final_msg = result[\"messages\"][-1].content\n",
        "    resp = process_response(final_msg)\n",
        "\n",
        "    root_cause = resp.get(\"root_cause\")\n",
        "    reasoning = resp.get(\"reasoning\")\n",
        "\n",
        "    # ---------- Step 2: Extract tool-call trace ----------\n",
        "\n",
        "    internal_msgs = result[\"messages\"][2:-1]\n",
        "\n",
        "    tool_call_msgs = [\n",
        "        m for m in internal_msgs\n",
        "        if (\n",
        "            (isinstance(m, AIMessage) and getattr(m, \"tool_calls\", None))\n",
        "            or isinstance(m, ToolMessage)\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    structured_trace_entry = {\n",
        "        \"agent\": \"RootCauseAnalysisAgent\",\n",
        "        \"step\": \"Generated structured root cause\",\n",
        "        \"calls\": serialize_messages(tool_call_msgs),\n",
        "        \"root_cause\": root_cause\n",
        "    }\n",
        "\n",
        "    #print(json.dumps(structured_trace_entry, indent=3))\n",
        "\n",
        "\n",
        "    # ---------- Step 4: Return final output ----------\n",
        "\n",
        "    return {\n",
        "        \"root_cause\": root_cause,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"trace\": [\n",
        "            structured_trace_entry\n",
        "        ]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG8LvkEnjb0Q"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain.tools import tool\n",
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "from typing import Dict, Any, List\n",
        "import json\n",
        "\n",
        "\n",
        "rca_report_agent = create_agent(\n",
        "    model=get_llm_model(),\n",
        "    tools=[\n",
        "    ],\n",
        "    middleware=[handle_tool_errors],\n",
        "    store=store,\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "\n",
        "@tool\n",
        "def rca_report_agent_tool(\n",
        "    root_cause: str,\n",
        "    reasoning: str,\n",
        "    user_id: str,\n",
        "    query_id: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Purpose:\n",
        "        Produce the final human-readable report.\n",
        "\n",
        "    When to use:\n",
        "        Use this tool as the final step of an RCA workflow to generate a\n",
        "        human-readable report.\n",
        "\n",
        "    Inputs:\n",
        "        - root_cause (str): root cause\n",
        "        - reasoning (str): reasoning\n",
        "        - user_id (str): User/session identifier for scoped memory access\n",
        "        - query_id (str): Query/thread identifier\n",
        "\n",
        "    Output:\n",
        "        - dict:\n",
        "            - \"report_text\": Human-readable RCA report\n",
        "            - \"trace\": Tool-call trace for observability\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------- Step 1: Generate human-readable RCA report ----------\n",
        "\n",
        "    report_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"\n",
        "You are an expert supply chain and demand planning analyst.\n",
        "\n",
        "Create a professional Root Cause Analysis Report.\n",
        "\n",
        "Audience:\n",
        "- Demand Planning\n",
        "- Inventory Management\n",
        "- Supply Chain Teams\n",
        "\n",
        "Requirements:\n",
        "- Clear structured sections\n",
        "- Bullet points where appropriate\n",
        "- No JSON, no code\n",
        "- Pure narrative report\n",
        "\n",
        "The report MUST include:\n",
        "- Executive Summary\n",
        "- Primary Root Cause(s)\n",
        "- Supporting Evidence\n",
        "- Contributing Factors\n",
        "- Key Data Points\n",
        "- Timeline of Events\n",
        "- Recommendations\n",
        "- Final Conclusion\n",
        "\n",
        "Tone:\n",
        "Analytical, data-driven, formal, concise.\n",
        "\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "Use the following structured RCA output:\n",
        "\n",
        "{json.dumps(root_cause, indent=2)}\n",
        "{json.dumps(reasoning, indent=2)}\n",
        "\"\"\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    config = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": user_id,\n",
        "            \"thread_id\": query_id\n",
        "        }\n",
        "    }\n",
        "\n",
        "    report_text = rca_report_agent.invoke(report_messages, config).content\n",
        "\n",
        "    report_trace_entry = {\n",
        "        \"agent\": \"RootCauseAnalysisAgent\",\n",
        "        \"step\": \"Generated RCA report\",\n",
        "        \"report_text\": report_text\n",
        "    }\n",
        "\n",
        "    # ---------- Step 4: Return final output ----------\n",
        "\n",
        "    return {\n",
        "        \"report_text\": report_text,\n",
        "        \"trace\": [\n",
        "            report_trace_entry\n",
        "        ]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLobBeRJTU4h"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "reflection_prompt_template = \"\"\"\n",
        "You are analyzing conversations from a supply-chain Root Cause Analysis (RCA) assistant to create episodic memories that will improve future RCA interactions.\n",
        "\n",
        "Your task is to extract the most useful, reusable insights from the conversation that would help when handling similar RCA scenarios in the future.\n",
        "\n",
        "Review the conversation and create a memory reflection following these rules:\n",
        "\n",
        "1. For any field where information is missing or not applicable, use \"N/A\"\n",
        "2. Be extremely concise  each string must be one clear, actionable sentence\n",
        "3. Focus only on information that improves future RCA effectiveness\n",
        "4. Context_tags must be specific enough to match similar RCA situations but general enough to be reusable\n",
        "\n",
        "Output valid JSON in exactly this format:\n",
        "{{\n",
        "    \"context_tags\": [               // 24 keywords identifying similar RCA scenarios\n",
        "        string,                     // Use domain-specific terms like \"sales_decline\", \"inventory_stockout\", \"logistics_delay\", \"forecast_bias\"\n",
        "        ...\n",
        "    ],\n",
        "    \"conversation_summary\": string, // One sentence describing what RCA problem was addressed and resolved\n",
        "    \"what_worked\": string,          // Most effective RCA technique or reasoning strategy used\n",
        "    \"what_to_avoid\": string         // Key RCA pitfall or ineffective approach to avoid in future\n",
        "}}\n",
        "\n",
        "Good context_tags examples:\n",
        "- [\"sales_decline\", \"regional_performance\", \"pricing_impact\"]\n",
        "- [\"inventory_stockout\", \"demand_supply_mismatch\", \"replenishment_delay\"]\n",
        "- [\"logistics_delay\", \"carrier_performance\", \"lead_time_variance\"]\n",
        "- [\"forecast_bias\", \"seasonality_error\", \"demand_planning\"]\n",
        "\n",
        "Bad context_tags examples:\n",
        "- [\"supply_chain\", \"rca\", \"analysis\"]\n",
        "- [\"problem\", \"issue\", \"data\"]\n",
        "\n",
        "Good conversation_summary examples:\n",
        "- \"Identified delayed replenishment and regional logistics constraints as root causes of declining sales in the South region\"\n",
        "- \"Analyzed forecast bias caused by seasonality misalignment leading to excess inventory buildup\"\n",
        "\n",
        "Bad conversation_summary examples:\n",
        "- \"Discussed a supply chain problem\"\n",
        "- \"Did root cause analysis\"\n",
        "\n",
        "Good what_worked examples:\n",
        "- \"Sequentially ruling out demand and inventory issues before focusing on logistics lead-time variance\"\n",
        "- \"Using historical sales and inventory trends to validate and eliminate hypotheses systematically\"\n",
        "\n",
        "Bad what_worked examples:\n",
        "- \"Analyzed the data\"\n",
        "- \"Used RCA techniques\"\n",
        "\n",
        "Good what_to_avoid examples:\n",
        "- \"Jumping to conclusions without validating hypotheses against sales and inventory data\"\n",
        "- \"Focusing on a single function without checking cross-functional dependencies\"\n",
        "\n",
        "Do not include any text outside the JSON object in your response.\n",
        "\n",
        "Here is the prior conversation:\n",
        "\n",
        "{conversation}\n",
        "\"\"\"\n",
        "reflection_prompt = ChatPromptTemplate.from_template(reflection_prompt_template)\n",
        "\n",
        "reflect = reflection_prompt | get_llm_model() | JsonOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmHFpzUlTfyt"
      },
      "outputs": [],
      "source": [
        "def format_conversation(history):\n",
        "\n",
        "    # Create an empty list placeholder\n",
        "    conversation = []\n",
        "\n",
        "    for message in history:\n",
        "      role = \"\"\n",
        "      content = \"\"\n",
        "      if isinstance(message, BaseMessage) or isinstance(message, HumanMessage) or isinstance(message, AIMessage):\n",
        "            role = message.type.upper()\n",
        "            content = message.content\n",
        "      conversation.append(f\"{role}: {content}\")\n",
        "\n",
        "    # Join with newlines\n",
        "    return \"\\n\".join(conversation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfYlqjT3X0nW"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "def add_episodic_memory(rca_state, config):\n",
        "    \"\"\"\n",
        "    Creates and stores episodic memory for an RCA conversation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Guard: history must exist and be meaningful\n",
        "    history = rca_state.get(\"history\")\n",
        "    if not history:\n",
        "        return\n",
        "\n",
        "    # Convert messages  readable conversation text\n",
        "    conversation = format_conversation(history)\n",
        "\n",
        "    # Generate reflection\n",
        "    reflection = reflect.invoke({\n",
        "        \"conversation\": conversation\n",
        "    })\n",
        "\n",
        "    reflection[\"conversation\"] = conversation\n",
        "\n",
        "    print(\"episodic reflection ===\", reflection)\n",
        "\n",
        "    # Store episodic memory\n",
        "    store.put(\n",
        "        namespace=(\"episodic\", config[\"configurable\"][\"user_id\"]),\n",
        "        key=f\"episodic_rca_{uuid.uuid4().hex}\",\n",
        "        value=reflection\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important**\n",
        "\n",
        "Episodic memory alone does not scale agents. It is only good at remembering what happened. Not yet good at remembering how to act better next time. Procedural + Semantic abstraction is what makes agents improve over time."
      ],
      "metadata": {
        "id": "u_-A87q1pmLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What procedural memory is**\n",
        "\n",
        "Stable strategies\n",
        "\n",
        "Decision heuristics\n",
        "\n",
        "Tool-usage patterns\n",
        "\n",
        "Ordering of steps that worked well\n",
        "\n",
        "**Example procedural memories:**\n",
        "\n",
        "Always validate demand-side hypotheses before inventory\n",
        "\n",
        "Promo-related RCAs should check replenishment timing first\n",
        "\n",
        "If sales spike + stockout  check emergency receipts"
      ],
      "metadata": {
        "id": "nnJVUUCBp5ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "procedural_reflection_template = \"\"\"\n",
        "You are extracting PROCEDURAL MEMORY for an RCA agent.\n",
        "\n",
        "Focus ONLY on reusable process knowledge.\n",
        "\n",
        "Extract:\n",
        "1. When to use which agent\n",
        "2. Ordering of analysis steps\n",
        "3. Tool usage heuristics\n",
        "4. Decision rules\n",
        "\n",
        "Output JSON:\n",
        "{{\n",
        "  \"procedure_name\": \"string\",\n",
        "  \"applicable_when\": \"string\",\n",
        "  \"steps\": [\"step1\", \"step2\", \"...\"],\n",
        "  \"tool_heuristics\": [\"rule1\", \"rule2\"]\n",
        "}}\n",
        "Conversation:\n",
        "{conversation}\n",
        "\"\"\"\n",
        "procedural_reflection_prompt = ChatPromptTemplate.from_template(procedural_reflection_template)\n",
        "\n",
        "procedural_reflection = procedural_reflection_prompt | get_llm_model() | JsonOutputParser()"
      ],
      "metadata": {
        "id": "RwGnOUKGqMMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "def add_procedural_memory(rca_state, config):\n",
        "    \"\"\"\n",
        "    Creates and stores procedural memory for an RCA conversation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Guard: history must exist and be meaningful\n",
        "    history = rca_state.get(\"history\")\n",
        "    if not history:\n",
        "        return\n",
        "\n",
        "    # Convert messages  readable conversation text\n",
        "    conversation = format_conversation(history)\n",
        "\n",
        "    # Generate reflection\n",
        "    reflection = procedural_reflection.invoke({\n",
        "        \"conversation\": conversation\n",
        "    })\n",
        "\n",
        "    print(\"procedural_reflection ===\", reflection)\n",
        "\n",
        "    # Store procedural memory\n",
        "    store.put(\n",
        "        namespace=(\"procedural\", config[\"configurable\"][\"user_id\"]),\n",
        "        key=f\"procedural_rca_{uuid.uuid4().hex}\",\n",
        "        value=reflection\n",
        "    )\n"
      ],
      "metadata": {
        "id": "RdSh25jVsFXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Semantic Abstraction**\n",
        "multiple episodic memories  generalized knowledge.\n",
        "This runs periodically, not per request"
      ],
      "metadata": {
        "id": "_D81UKgMS0IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_abstraction_prompt = \"\"\"\n",
        "You are building SEMANTIC MEMORY for an RCA agent.\n",
        "\n",
        "Given multiple episodic RCA reflections, extract generalized,\n",
        "reusable knowledge that holds across cases.\n",
        "\n",
        "Rules:\n",
        "- Do NOT mention specific dates, stores, or conversations\n",
        "- Focus on patterns, causal relationships, and general truths\n",
        "- One semantic fact should apply to many future RCA cases\n",
        "\n",
        "Output ONLY valid JSON in this format:\n",
        "{{\n",
        "  \"semantic_fact\": \"string\",\n",
        "  \"applicable_context\": [\"keyword1\", \"keyword2\"],\n",
        "  \"confidence\": \"low | medium | high\"\n",
        "}}\n",
        "\n",
        "Episodic memories:\n",
        "{episodes}\n",
        "\"\"\"\n",
        "semantic_reflection_prompt = ChatPromptTemplate.from_template(\n",
        "    semantic_abstraction_prompt\n",
        ")\n",
        "\n",
        "semantic_reflection_chain = (\n",
        "    semantic_reflection_prompt\n",
        "    | get_llm_model()\n",
        "    | JsonOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "hVIOlP7sP_A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_semantic_memory(user_id: str, query: str, store, llm, min_episodes: int = 3):\n",
        "    \"\"\"\n",
        "    Distill episodic RCA memories into semantic memory.\n",
        "    \"\"\"\n",
        "\n",
        "    # Pull recent episodic memories\n",
        "    namespace=(\"episodic\", user_id)\n",
        "    episodic = store.search(\n",
        "        namespace,\n",
        "        query=query,\n",
        "        limit=10\n",
        "    )\n",
        "\n",
        "    if len(episodic) < min_episodes:\n",
        "        return None  # not enough signal yet\n",
        "\n",
        "    # Prepare episodes text\n",
        "    episodes_text = []\n",
        "    for e in episodic:\n",
        "        v = e.value\n",
        "        episodes_text.append(\n",
        "            f\"- Summary: {v.get('conversation_summary')}\\n\"\n",
        "            f\"  Worked: {v.get('what_worked')}\\n\"\n",
        "            f\"  Avoid: {v.get('what_to_avoid')}\"\n",
        "        )\n",
        "\n",
        "    # ---------------------------\n",
        "    # 3. Invoke semantic chain\n",
        "    # ---------------------------\n",
        "    semantic = semantic_reflection_chain.invoke({\n",
        "        \"episodes\": \"\\n\".join(episodes_text)\n",
        "    })\n",
        "\n",
        "    print(\"semantic_reflection ===\", semantic)\n",
        "\n",
        "    if not semantic or not isinstance(semantic, dict):\n",
        "        return None\n",
        "\n",
        "    # What usefulness means here A memory is useful if:It was retrieved And the RCA completed successfully\n",
        "    semantic[\"usefulness\"] = 0\n",
        "    semantic[\"last_used_at\"] = time.time()\n",
        "\n",
        "    # Store semantic memory\n",
        "    store.put(\n",
        "        namespace=(\"semantic\", user_id),\n",
        "        key=f\"semantic_{uuid.uuid4().hex}\",\n",
        "        value=semantic\n",
        "    )\n",
        "\n",
        "    return semantic\n"
      ],
      "metadata": {
        "id": "k97zr5pkRMam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mark_memory_useful(memories):\n",
        "    for m in memories:\n",
        "        m.value[\"usefulness\"] = m.value.get(\"usefulness\", 0) + 1"
      ],
      "metadata": {
        "id": "x5OhoIRLNPKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "5qyiZyHzimzQ",
        "outputId": "4a964e65-8487-4f40-c54d-fd2402d8ea42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTodoListMiddleware()\\n  Equip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following:\\n    Complex multi-step tasks requiring coordination across multiple tools.\\n    Long-running operations where progress visibility is important.\\n  This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning.\\n\\nContext management: FilesystemMiddleware()\\n    File system tools (ls, read_file, write_file, edit_file) allow agents to offload large context to memory,\\n    preventing context window overflow and enabling work with variable-length tool results.\\n    By default, these tools write to a local filesystem in graph state.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "'''\n",
        "TodoListMiddleware()\n",
        "  Equip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following:\n",
        "    Complex multi-step tasks requiring coordination across multiple tools.\n",
        "    Long-running operations where progress visibility is important.\n",
        "  This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning.\n",
        "\n",
        "Context management: FilesystemMiddleware()\n",
        "    File system tools (ls, read_file, write_file, edit_file) allow agents to offload large context to memory,\n",
        "    preventing context window overflow and enabling work with variable-length tool results.\n",
        "    By default, these tools write to a local filesystem in graph state.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSD-M6ty1Sxx"
      },
      "outputs": [],
      "source": [
        "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
        "import json\n",
        "from langchain.agents import create_agent\n",
        "\n",
        "\n",
        "\n",
        "router_agent = create_agent(\n",
        "    model=get_llm_model(),\n",
        "    tools=[\n",
        "        create_search_memory_tool(\n",
        "            namespace=(\"orchestration\", \"{user_id}\")\n",
        "        ),\n",
        "        create_manage_memory_tool(\n",
        "            namespace=(\"orchestration\", \"{user_id}\")\n",
        "        ),\n",
        "        hypothesis_agent_tool,\n",
        "        sales_analysis_agent_tool,\n",
        "        inventory_analysis_agent_tool,\n",
        "        hypothesis_validation_agent_tool,\n",
        "        root_cause_analysis_agent_tool,\n",
        "        rca_report_agent_tool\n",
        "    ],\n",
        "\n",
        "    middleware=[handle_tool_errors, TodoListMiddleware()],\n",
        "    store=store,\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "def orchestration_agent(rca_state, config):\n",
        "\n",
        "    #  Ensure history exists\n",
        "    if \"history\" not in rca_state or rca_state[\"history\"] is None:\n",
        "        print(\"***Empty History***\")\n",
        "        rca_state[\"history\"] = []\n",
        "\n",
        "    #  Build memory-augmented context\n",
        "    #  Inject sementic abstract + procedural + episodic + conversation context\n",
        "    memory_context = build_memory_augmented_prompt(\n",
        "        query=rca_state[\"task\"],\n",
        "        state=rca_state,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "                {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"\n",
        "        You are a Deep Research Agent.\n",
        "\n",
        "        Task: {rca_state[\"task\"]}\n",
        "\n",
        "        User Id: {config[\"configurable\"][\"user_id\"]}\n",
        "\n",
        "        Query Id: {config[\"configurable\"][\"thread_id\"]}\n",
        "\n",
        "        Use the following sementic abstract + procedural + episodic + conversation context:\n",
        "        Memory Context(memory_context):'''{memory_context}'''\n",
        "\n",
        "        Your role is to analyze the user's input, determine the appropriate\n",
        "        research or response strategy, and use the available tools to resolve\n",
        "        the request.\n",
        "\n",
        "        The set of tools available to you may change dynamically.\n",
        "        You must infer what each tool does from its description.\n",
        "\n",
        "        ------------------------------------------------------------\n",
        "        CORE RESPONSIBILITIES:\n",
        "\n",
        "        1. Understand User Intent\n",
        "          - The user input may be:\n",
        "             a greeting or help request (e.g., \"hi\", \"hello\", \"help\")\n",
        "             a general question\n",
        "             a root cause analysis or supply chain investigation\n",
        "          - Do not assume the input is analytical.\n",
        "\n",
        "        2. Decide the Level of Depth Required\n",
        "          - If the input can be addressed with a simple explanation or response,\n",
        "            prefer a lightweight approach.\n",
        "          - If the input requires investigation, reasoning, or analysis,\n",
        "            proceed with deep research behavior.\n",
        "\n",
        "        3. Create an Internal Plan\n",
        "          - Before calling any tool, determine:\n",
        "             what information is missing\n",
        "             what needs to be discovered or generated\n",
        "             whether memory or prior context is relevant\n",
        "          - The plan does not need to be shown unless required by a tool.\n",
        "\n",
        "        4. Execute Using Tools\n",
        "          - Use the **todo's** tools to carry out the plan.\n",
        "          - Choose tools based on their descriptions, not their names.\n",
        "          - You may call multiple tools if necessary.\n",
        "          - Always prefer the minimal set of tool calls needed.\n",
        "\n",
        "        5. RCA-Specific Behavior (when applicable)\n",
        "          - When the task involves diagnosing causes of a problem:\n",
        "             avoid jumping to conclusions\n",
        "             favor hypothesis generation before validation\n",
        "             rely on state, memory, and evidence\n",
        "\n",
        "        ------------------------------------------------------------\n",
        "        IMPORTANT RULES:\n",
        "\n",
        "        - Do not hard-code assumptions about tool availability.\n",
        "        - Do not invent tools or capabilities.\n",
        "        - Do not answer complex questions directly in free text\n",
        "          if an appropriate tool exists.\n",
        "        - Be robust to vague, short, or conversational user inputs.\n",
        "        - Think first, then act through tools.\n",
        "\n",
        "        You are expected to behave as a flexible, adaptive\n",
        "        deep-research agent, not a fixed pipeline.\n",
        "        \"\"\"\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": rca_state[\"task\"] }\n",
        "    ]\n",
        "\n",
        "    #print(\"messages==\", messages)\n",
        "    config = {\n",
        "        **config,\n",
        "        \"configurable\": {\n",
        "            **config.get(\"configurable\", {}),\n",
        "            \"rca_state\": rca_state\n",
        "        }\n",
        "    }\n",
        "\n",
        "    result = router_agent.invoke({\"messages\": messages}, config)\n",
        "\n",
        "    final_msg = result[\"messages\"][-1].content\n",
        "    # final message from the agent\n",
        "    #output = process_response(final_msg)\n",
        "    output = final_msg\n",
        "\n",
        "    # Extract internal tool-call reasoning messages\n",
        "    # Skip system(0), user(1), final(-1)\n",
        "    internal_msgs = result[\"messages\"][2:-1]\n",
        "    tool_call_msgs = [\n",
        "        m for m in internal_msgs\n",
        "        if (\n",
        "            (isinstance(m, AIMessage) and getattr(m, \"tool_calls\", None))\n",
        "            or isinstance(m, ToolMessage)\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    trace_entry = {\n",
        "        \"agent\": \"Orchestration Agent\",\n",
        "        \"tool_calls\": serialize_messages(tool_call_msgs)\n",
        "    }\n",
        "\n",
        "    #print(json.dumps(trace_entry, indent=3))\n",
        "\n",
        "    rca_state[\"output\"] = output\n",
        "    rca_state[\"trace\"] = trace_entry\n",
        "\n",
        "    append_rca_history(rca_state)\n",
        "\n",
        "    return rca_state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Minimal inspector function\n",
        "def inspect_memory(store, user_id):\n",
        "\n",
        "    print(\"\\n--------------------------------------------------------------------------\")\n",
        "    print(\"memory inspector\")\n",
        "    report = {}\n",
        "\n",
        "    for layer in [\"episodic\", \"procedural\", \"semantic\"]:\n",
        "        namespace=(layer, user_id)\n",
        "        memories = store.search(\n",
        "            namespace,\n",
        "            limit=10\n",
        "        )\n",
        "\n",
        "        report[layer] = [\n",
        "            {\n",
        "                \"key\": m.key,\n",
        "                \"confidence\": m.value.get(\"confidence\"),\n",
        "                \"usefulness\": m.value.get(\"usefulness\", 0),\n",
        "                \"summary\": (\n",
        "                    m.value.get(\"conversation_summary\")\n",
        "                    or m.value.get(\"semantic_fact\")\n",
        "                    or m.value.get(\"procedure_name\")\n",
        "                )\n",
        "            }\n",
        "            for m in memories\n",
        "        ]\n",
        "    print(\"--------------------------------------------------------------------------\")\n",
        "    return report\n"
      ],
      "metadata": {
        "id": "4B7oWrq8Pe6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PEKh3QNKOGR",
        "outputId": "4c6b819e-a4a6-4335-bad4-3e12b237ca87",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " RCA Chatbot (type 'exit' to quit)\n",
            "\n",
            "\n",
            "======================================================================\n",
            "----------------------------------------------------------------------\n",
            "You      : Investigate why stores are expeciecning stockouts\n",
            "\n",
            "----------------------------------------------------------------------\n",
            " RCA Bot is thinking...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            " RCA Bot Answer\n",
            "----------------------------------------------------------------------\n",
            "Here is the Root Cause Analysis report for the stockouts experienced by stores:\n",
            "\n",
            "Executive Summary:\n",
            "Stores experienced stockouts primarily due to unexpectedly high demand driven by a promotional event, combined with delays in inventory replenishment and disruptions in supply chain operations. Uneven stock allocation across stores and additional stock shrinkage further exacerbated shortages.\n",
            "\n",
            "Primary Root Causes:\n",
            "- Unexpectedly high demand exceeding forecasted sales caused by promotional activities.\n",
            "- Delays and disruptions in inventory replenishment and shipments leading to stock shortages.\n",
            "- Inefficient distribution and allocation resulting in uneven stock levels across stores.\n",
            "- Supply chain disruptions affecting timely product availability.\n",
            "\n",
            "Supporting Evidence:\n",
            "- Sales data showed daily sales surged from an average of 2-6 units per store before the promotion to 15-55 units during the promotional period (May 1 to May 7, 2024).\n",
            "- Inventory data revealed replenishment shipments to all stores were delayed by 2 days during the promotion.\n",
            "- An emergency transfer of 50 units from one store to another highlighted uneven stock distribution.\n",
            "- Stock shrinkage of 12 units was recorded at one store during the promotional period.\n",
            "- No evidence of point-of-sale or inventory system errors was found.\n",
            "\n",
            "Contributing Factors:\n",
            "- Promotional pricing and marketing campaigns driving sudden spikes in demand.\n",
            "- Stock mismanagement and shrinkage at the store level.\n",
            "- Operational strain impacting inventory visibility during periods of delayed replenishment and emergency stock transfers.\n",
            "\n",
            "Key Data Points:\n",
            "- Promotion period: May 1 to May 7, 2024.\n",
            "- Sales volume during promotion ranged from 116 to 192 units per store.\n",
            "- Replenishment shipment delays of 2 days across all stores.\n",
            "- Emergency stock transfer of 50 units between stores.\n",
            "- Recorded shrinkage of 12 units at one store.\n",
            "\n",
            "Timeline of Events:\n",
            "- May 1, 2024: Promotion launched; daily sales surged significantly.\n",
            "- May 1, 2024: Emergency stock transfer of 50 units between stores.\n",
            "- May 3, 2024: Replenishment shipments delayed by 2 days.\n",
            "- May 4, 2024: Shrinkage adjustment of 12 units recorded.\n",
            "- May 7, 2024: Promotion ended.\n",
            "\n",
            "Recommendations:\n",
            "- Improve demand forecasting models to better anticipate promotional demand spikes.\n",
            "- Enhance supply chain processes to minimize replenishment delays and establish contingency plans for promotional periods.\n",
            "- Optimize stock allocation and distribution to ensure balanced inventory across stores before promotions.\n",
            "- Implement tighter controls and monitoring to reduce shrinkage during high-demand periods.\n",
            "- Strengthen inventory visibility and operational coordination during promotions for proactive stock management.\n",
            "\n",
            "Final Conclusion:\n",
            "The stockouts experienced by stores during the promotional period were driven by a combination of unexpectedly high demand and supply chain challenges, including replenishment delays and uneven stock distribution. Addressing these root causes with improved forecasting, supply chain resilience, and inventory management practices will help prevent similar issues in future promotions.\n",
            "{'agent': 'Orchestration Agent', 'tool_calls': [{'type': 'AIMessage', 'content': '', 'tool_calls': [{'name': 'write_todos', 'args': {'todos': [{'content': 'Generate plausible root-cause hypotheses for stores experiencing stockouts.', 'status': 'in_progress'}]}, 'id': 'call_ij0SlqrwGP7iwTR1zbgy0H83'}]}, {'type': 'ToolMessage', 'content': \"Updated todo list to [{'content': 'Generate plausible root-cause hypotheses for stores experiencing stockouts.', 'status': 'in_progress'}]\", 'tool_call_id': 'call_ij0SlqrwGP7iwTR1zbgy0H83'}, {'type': 'AIMessage', 'content': '', 'tool_calls': [{'name': 'hypothesis_agent_tool', 'args': {'task': 'Investigate why stores are experiencing stockouts', 'user_id': '2', 'query_id': '2', 'memory_context': \"You are an RCA assistant with access to memory.\\n\\n\\nGeneralized RCA knowledge:\\n- When direct access to critical sales or inventory data is unavailable during root cause analysis, generating and categorizing plausible common causes enables effective hypothesis generation and guides targeted data validation. (confidence: high)\\n\\n\\nNo relevant RCA procedures found.\\n\\n\\nSimilar past RCA experience:\\n- Current Conversation Match: HUMAN: Investigate why stores are experiencing stockouts during the TechGadget Pro promotion\\nAI: There seems to be a technical issue preventing me from using some tools right now. However, I can still help by outlining common plausible root causes for stockouts during a promotion like TechGadget Pro:\\n\\n1. Unexpectedly high demand exceeding forecasted sales.\\n2. Delays or issues in inventory replenishment or shipments.\\n3. Distribution or allocation problems causing uneven stock levels across stores.\\n4. Point-of-sale or inventory system errors causing inaccurate stock visibility.\\n5. Supply chain disruptions affecting product availability.\\n6. Promotional pricing or marketing driving demand spikes.\\n7. Stock mismanagement or shrinkage at stores.\\n\\nPlease let me know if you want me to analyze available sales or inventory data to further validate these or if you have access to specific data sources I could help you interpret.\\nUSER: hi\\nAI: Hello! How can I assist you today?\\nUSER: Investigate why stores are facing stockouts\\nAI: It seems there is a technical issue preventing me from using some tools right now. However, I can help by outlining common plausible root causes for stockouts in stores:\\n\\n1. Unexpectedly high demand exceeding forecasted sales.\\n2. Delays or issues in inventory replenishment or shipments.\\n3. Distribution or allocation problems causing uneven stock levels across stores.\\n4. Point-of-sale or inventory system errors causing inaccurate stock visibility.\\n5. Supply chain disruptions affecting product availability.\\n6. Promotional pricing or marketing driving demand spikes.\\n7. Stock mismanagement or shrinkage at stores.\\n\\nIf you have specific sales or inventory data available, I can help you interpret it to further validate these causes. Please let me know how you'd like to proceed.\\nUSER: investigate why store facing stockouts\\nAI: It appears there is a technical issue preventing me from using some analysis tools right now. However, I can still help by outlining common plausible root causes for store stockouts based on typical scenarios:\\n\\n1. Unexpectedly high demand exceeding forecasted sales.\\n2. Delays or issues in inventory replenishment or shipments.\\n3. Distribution or allocation problems causing uneven stock levels across stores.\\n4. Point-of-sale or inventory system errors causing inaccurate stock visibility.\\n5. Supply chain disruptions affecting product availability.\\n6. Promotional pricing or marketing driving demand spikes.\\n7. Stock mismanagement or shrinkage at stores.\\n\\nIf you have specific sales or inventory data available, you can share it with me, and I can help you interpret it to further validate these causes. Please let me know how you'd like to proceed.\\n\\nInstructions:\\n- Follow relevant procedures first\\n- Use past experiences to avoid known pitfalls\\n- Use recent conversation context for continuity\"}, 'id': 'call_dPGrpgFhoaSzRd8K9XeKaSHK'}]}, {'type': 'ToolMessage', 'content': '{\"hypotheses\": [\"Unexpectedly high demand exceeding forecasted sales causing depletion of stock.\", \"Delays or disruptions in inventory replenishment or shipments leading to stock shortages.\", \"Inefficient distribution or allocation causing uneven stock levels across different stores.\", \"Errors or inaccuracies in point-of-sale or inventory management systems resulting in poor stock visibility.\", \"Supply chain disruptions affecting the availability of products.\", \"Promotional activities or pricing strategies driving sudden spikes in demand.\", \"Stock mismanagement or shrinkage such as theft, damage, or administrative errors at store level.\"], \"trace\": [{\"agent\": \"HypothesisAgent\", \"step\": \"Generated hypotheses\", \"calls\": [], \"hypotheses\": [\"Unexpectedly high demand exceeding forecasted sales causing depletion of stock.\", \"Delays or disruptions in inventory replenishment or shipments leading to stock shortages.\", \"Inefficient distribution or allocation causing uneven stock levels across different stores.\", \"Errors or inaccuracies in point-of-sale or inventory management systems resulting in poor stock visibility.\", \"Supply chain disruptions affecting the availability of products.\", \"Promotional activities or pricing strategies driving sudden spikes in demand.\", \"Stock mismanagement or shrinkage such as theft, damage, or administrative errors at store level.\"]}]}', 'tool_call_id': 'call_dPGrpgFhoaSzRd8K9XeKaSHK'}, {'type': 'AIMessage', 'content': '', 'tool_calls': [{'name': 'write_todos', 'args': {'todos': [{'content': 'Generate plausible root-cause hypotheses for stores experiencing stockouts.', 'status': 'completed'}, {'content': 'Analyze sales data to validate hypotheses related to demand spikes and promotional impacts.', 'status': 'pending'}, {'content': 'Analyze inventory data to validate hypotheses related to replenishment delays, distribution issues, and stock mismanagement.', 'status': 'pending'}]}, 'id': 'call_hDqnEZlN86DXFQOIqvydBLAu'}]}, {'type': 'ToolMessage', 'content': \"Updated todo list to [{'content': 'Generate plausible root-cause hypotheses for stores experiencing stockouts.', 'status': 'completed'}, {'content': 'Analyze sales data to validate hypotheses related to demand spikes and promotional impacts.', 'status': 'pending'}, {'content': 'Analyze inventory data to validate hypotheses related to replenishment delays, distribution issues, and stock mismanagement.', 'status': 'pending'}]\", 'tool_call_id': 'call_hDqnEZlN86DXFQOIqvydBLAu'}, {'type': 'AIMessage', 'content': '', 'tool_calls': [{'name': 'sales_analysis_agent_tool', 'args': {'task': 'Investigate why stores are experiencing stockouts', 'hypotheses': ['Unexpectedly high demand exceeding forecasted sales causing depletion of stock.', 'Delays or disruptions in inventory replenishment or shipments leading to stock shortages.', 'Inefficient distribution or allocation causing uneven stock levels across different stores.', 'Errors or inaccuracies in point-of-sale or inventory management systems resulting in poor stock visibility.', 'Supply chain disruptions affecting the availability of products.', 'Promotional activities or pricing strategies driving sudden spikes in demand.', 'Stock mismanagement or shrinkage such as theft, damage, or administrative errors at store level.'], 'user_id': '2', 'query_id': '2', 'memory_context': \"You are an RCA assistant with access to memory.\\n\\n\\nGeneralized RCA knowledge:\\n- When direct access to critical sales or inventory data is unavailable during root cause analysis, generating and categorizing plausible common causes enables effective hypothesis generation and guides targeted data validation. (confidence: high)\\n\\n\\nNo relevant RCA procedures found.\\n\\n\\nSimilar past RCA experience:\\n- Current Conversation Match: HUMAN: Investigate why stores are experiencing stockouts during the TechGadget Pro promotion\\nAI: There seems to be a technical issue preventing me from using some tools right now. However, I can still help by outlining common plausible root causes for stockouts during a promotion like TechGadget Pro:\\n\\n1. Unexpectedly high demand exceeding forecasted sales.\\n2. Delays or issues in inventory replenishment or shipments.\\n3. Distribution or allocation problems causing uneven stock levels across stores.\\n4. Point-of-sale or inventory system errors causing inaccurate stock visibility.\\n5. Supply chain disruptions affecting product availability.\\n6. Promotional pricing or marketing driving demand spikes.\\n7. Stock mismanagement or shrinkage at stores.\\n\\nPlease let me know if you want me to analyze available sales or inventory data to further validate these or if you have access to specific data sources I could help you interpret.\\nUSER: hi\\nAI: Hello! How can I assist you today?\\nUSER: Investigate why stores are facing stockouts\\nAI: It seems there is a technical issue preventing me from using some tools right now. However, I can help by outlining common plausible root causes for stockouts in stores:\\n\\n1. Unexpectedly high demand exceeding forecasted sales.\\n2. Delays or issues in inventory replenishment or shipments.\\n3. Distribution or allocation problems causing uneven stock levels across stores.\\n4. Point-of-sale or inventory system errors causing inaccurate stock visibility.\\n5. Supply chain disruptions affecting product availability.\\n6. Promotional pricing or marketing driving demand spikes.\\n7. Stock mismanagement or shrinkage at stores.\\n\\nIf you have specific sales or inventory data available, I can help you interpret it to further validate these causes. Please let me know how you'd like to proceed.\\nUSER: investigate why store facing stockouts\\nAI: It appears there is a technical issue preventing me from using some analysis tools right now. However, I can still help by outlining common plausible root causes for store stockouts based on typical scenarios:\\n\\n1. Unexpectedly high demand exceeding forecasted sales.\\n2. Delays or issues in inventory replenishment or shipments.\\n3. Distribution or allocation problems causing uneven stock levels across stores.\\n4. Point-of-sale or inventory system errors causing inaccurate stock visibility.\\n5. Supply chain disruptions affecting product availability.\\n6. Promotional pricing or marketing driving demand spikes.\\n7. Stock mismanagement or shrinkage at stores.\\n\\nIf you have specific sales or inventory data available, you can share it with me, and I can help you interpret it to further validate these causes. Please let me know how you'd like to proceed.\\n\\nInstructions:\\n- Follow relevant procedures first\\n- Use past experiences to avoid known pitfalls\\n- Use recent conversation context for continuity\"}, 'id': 'call_dT450NR9Jjaa23y0dl2W0SWg'}, {'name': 'inventory_analysis_agent_tool', 'args': {'task': 'Investigate why stores are experiencing stockouts', 'hypotheses': ['Unexpectedly high demand exceeding forecasted sales causing depletion of stock.', 'Delays or disruptions in inventory replenishment or shipments leading to stock shortages.', 'Inefficient distribution or allocation causing uneven stock levels across different stores.', 'Errors or inaccuracies in point-of-sale or inventory management systems resulting in poor stock visibility.', 'Supply chain disruptions affecting the availability of products.', 'Promotional activities or pricing strategies driving sudden spikes in demand.', 'Stock mismanagement or shrinkage such as theft, damage, or administrative errors at store level.'], 'user_id': '2', 'query_id': '2', 'memory_context': \"You are an RCA assistant with access to memory.\\n\\n\\nGeneralized RCA knowledge:\\n- When direct access to critical sales or inventory data is unavailable during root cause analysis, generating and categorizing plausible common causes enables effective hypothesis generation and guides targeted data validation. (confidence: high)\\n\\n\\nNo relevant RCA procedures found.\\n\\n\\nSimilar past RCA experience:\\n- Current Conversation Match: HUMAN: Investigate why stores are experiencing stockouts during the TechGadget Pro promotion\\nAI: There seems to be a technical issue preventing me from using some tools right now. However, I can still help by outlining common plausible root causes for stockouts during a promotion like TechGadget Pro:\\n\\n1. Unexpectedly high demand exceeding forecasted sales.\\n2. Delays or issues in inventory replenishment or shipments.\\n3. Distribution or allocation problems causing uneven stock levels across stores.\\n4. Point-of-sale or inventory system errors causing inaccurate stock visibility.\\n5. Supply chain disruptions affecting product availability.\\n6. Promotional pricing or marketing driving demand spikes.\\n7. Stock mismanagement or shrinkage at stores.\\n\\nPlease let me know if you want me to analyze available sales or inventory data to further validate these or if you have access to specific data sources I could help you interpret.\\nUSER: hi\\nAI: Hello! How can I assist you today?\\nUSER: Investigate why stores are facing stockouts\\nAI: It seems there is a technical issue preventing me from using some tools right now. However, I can help by outlining common plausible root causes for stockouts in stores:\\n\\n1. Unexpectedly high demand exceeding forecasted sales.\\n2. Delays or issues in inventory replenishment or shipments.\\n3. Distribution or allocation problems causing uneven stock levels across stores.\\n4. Point-of-sale or inventory system errors causing inaccurate stock visibility.\\n5. Supply chain disruptions affecting product availability.\\n6. Promotional pricing or marketing driving demand spikes.\\n7. Stock mismanagement or shrinkage at stores.\\n\\nIf you have specific sales or inventory data available, I can help you interpret it to further validate these causes. Please let me know how you'd like to proceed.\\nUSER: investigate why store facing stockouts\\nAI: It appears there is a technical issue preventing me from using some analysis tools right now. However, I can still help by outlining common plausible root causes for store stockouts based on typical scenarios:\\n\\n1. Unexpectedly high demand exceeding forecasted sales.\\n2. Delays or issues in inventory replenishment or shipments.\\n3. Distribution or allocation problems causing uneven stock levels across stores.\\n4. Point-of-sale or inventory system errors causing inaccurate stock visibility.\\n5. Supply chain disruptions affecting the availability of products.\\n6. Promotional pricing or marketing driving demand spikes.\\n7. Stock mismanagement or shrinkage at stores.\\n\\nIf you have specific sales or inventory data available, you can share it with me, and I can help you interpret it to further validate these causes. Please let me know how you'd like to proceed.\\n\\nInstructions:\\n- Follow relevant procedures first\\n- Use past experiences to avoid known pitfalls\\n- Use recent conversation context for continuity\"}, 'id': 'call_Upr3rou4yHyYNpgpXS3uiftW'}]}, {'type': 'ToolMessage', 'content': '{\"sales_insights\": \"Sales data shows a significant surge in quantity sold across all stores starting from May 1, 2024, which aligns exactly with the promotional period from May 1 to May 7, 2024. Daily sales before the promotion averaged between 2 and 6 units per store, but during the promotion, these numbers jumped sharply to between 15 and 55 units per store daily. This confirms that the stores experienced unexpectedly high demand driven by promotional activities, supporting the hypotheses that the stockouts are due to demand exceeding forecasted sales and promotional-driven spikes.\", \"trace\": [{\"agent\": \"SalesAnalysisAgent\", \"step\": \"Validated sales hypotheses\", \"calls\": [{\"type\": \"AIMessage\", \"content\": \"\", \"tool_calls\": [{\"name\": \"get_daily_sales\", \"args\": {}, \"id\": \"call_wf0s2zTJ0LN7gtjJhlN2VDoY\"}, {\"name\": \"get_promo_period\", \"args\": {}, \"id\": \"call_jWuUTdXO1n4jWp7RueFIvUKq\"}]}, {\"type\": \"ToolMessage\", \"content\": \"[{\\'transaction_date\\': Timestamp(\\'2024-04-24 00:00:00\\'), \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'quantity_sold\\': 5}, {\\'transaction_date\\': Timestamp(\\'2024-04-24 00:00:00\\'), \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'quantity_sold\\': 3}, {\\'transaction_date\\': Timestamp(\\'2024-04-24 00:00:00\\'), \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'quantity_sold\\': 4}, {\\'transaction_date\\': Timestamp(\\'2024-04-24 00:00:00\\'), \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'quantity_sold\\': 2}, {\\'transaction_date\\': Timestamp(\\'2024-04-24 00:00:00\\'), \\'store_id\\': \\'S005\\', \\'store_name\\': \\'Southpark Store\\', \\'quantity_sold\\': 6}, {\\'transaction_date\\': Timestamp(\\'2024-04-25 00:00:00\\'), \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'quantity_sold\\': 4}, {\\'transaction_date\\': Timestamp(\\'2024-04-25 00:00:00\\'), \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'quantity_sold\\': 5}, {\\'transaction_date\\': Timestamp(\\'2024-04-25 00:00:00\\'), \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'quantity_sold\\': 3}, {\\'transaction_date\\': Timestamp(\\'2024-04-25 00:00:00\\'), \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'quantity_sold\\': 4}, {\\'transaction_date\\': Timestamp(\\'2024-04-25 00:00:00\\'), \\'store_id\\': \\'S005\\', \\'store_name\\': \\'Southpark Store\\', \\'quantity_sold\\': 3}, {\\'transaction_date\\': Timestamp(\\'2024-04-26 00:00:00\\'), \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'quantity_sold\\': 6}, {\\'transaction_date\\': Timestamp(\\'2024-04-26 00:00:00\\'), \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'quantity_sold\\': 4}, {\\'transaction_date\\': Timestamp(\\'2024-04-26 00:00:00\\'), \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'quantity_sold\\': 5}, {\\'transaction_date\\': Timestamp(\\'2024-04-26 00:00:00\\'), \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'quantity_sold\\': 3}, {\\'transaction_date\\': Timestamp(\\'2024-04-26 00:00:00\\'), \\'store_id\\': \\'S005\\', \\'store_name\\': \\'Southpark Store\\', \\'quantity_sold\\': 4}, {\\'transaction_date\\': Timestamp(\\'2024-04-27 00:00:00\\'), \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'quantity_sold\\': 5}, {\\'transaction_date\\': Timestamp(\\'2024-04-27 00:00:00\\'), \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'quantity_sold\\': 6}, {\\'transaction_date\\': Timestamp(\\'2024-04-27 00:00:00\\'), \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'quantity_sold\\': 4}, {\\'transaction_date\\': Timestamp(\\'2024-04-27 00:00:00\\'), \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'quantity_sold\\': 5}, {\\'transaction_date\\': Timestamp(\\'2024-04-27 00:00:00\\'), \\'store_id\\': \\'S005\\', \\'store_name\\': \\'Southpark Store\\', \\'quantity_sold\\': 5}, {\\'transaction_date\\': Timestamp(\\'2024-04-28 00:00:00\\'), \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'quantity_sold\\': 4}, {\\'transaction_date\\': Timestamp(\\'2024-04-28 00:00:00\\'), \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'quantity_sold\\': 5}, {\\'transaction_date\\': Timestamp(\\'2024-04-28 00:00:00\\'), \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'quantity_sold\\': 6}, {\\'transaction_date\\': Timestamp(\\'2024-04-28 00:00:00\\'), \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'quantity_sold\\': 4}, {\\'transaction_date\\': Timestamp(\\'2024-04-28 00:00:00\\'), \\'store_id\\': \\'S005\\', \\'store_name\\': \\'Southpark Store\\', \\'quantity_sold\\': 3}, {\\'transaction_date\\': Timestamp(\\'2024-04-29 00:00:00\\'), \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'quantity_sold\\': 5}, {\\'transaction_date\\': Timestamp(\\'2024-04-29 00:00:00\\'), \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'quantity_sold\\': 4}, {\\'transaction_date\\': Timestamp(\\'2024-04-29 00:00:00\\'), \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'quantity_sold\\': 5}, {\\'transaction_date\\': Timestamp(\\'2024-04-29 00:00:00\\'), \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'quantity_sold\\': 6}, {\\'transaction_date\\': Timestamp(\\'2024-04-29 00:00:00\\'), \\'store_id\\': \\'S005\\', \\'store_name\\': \\'Southpark Store\\', \\'quantity_sold\\': 4}, {\\'transaction_date\\': Timestamp(\\'2024-04-30 00:00:00\\'), \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'quantity_sold\\': 6}, {\\'transaction_date\\': Timestamp(\\'2024-04-30 00:00:00\\'), \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'quantity_sold\\': 5}, {\\'transaction_date\\': Timestamp(\\'2024-04-30 00:00:00\\'), \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'quantity_sold\\': 4}, {\\'transaction_date\\': Timestamp(\\'2024-04-30 00:00:00\\'), \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'quantity_sold\\': 5}, {\\'transaction_date\\': Timestamp(\\'2024-04-30 00:00:00\\'), \\'store_id\\': \\'S005\\', \\'store_name\\': \\'Southpark Store\\', \\'quantity_sold\\': 5}, {\\'transaction_date\\': Timestamp(\\'2024-05-01 00:00:00\\'), \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'quantity_sold\\': 45}, {\\'transaction_date\\': Timestamp(\\'2024-05-01 00:00:00\\'), \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'quantity_sold\\': 38}, {\\'transaction_date\\': Timestamp(\\'2024-05-01 00:00:00\\'), \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'quantity_sold\\': 42}, {\\'transaction_date\\': Timestamp(\\'2024-05-01 00:00:00\\'), \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'quantity_sold\\': 35}, {\\'transaction_date\\': Timestamp(\\'2024-05-01 00:00:00\\'), \\'store_id\\': \\'S005\\', \\'store_name\\': \\'Southpark Store\\', \\'quantity_sold\\': 40}, {\\'transaction_date\\': Timestamp(\\'2024-05-02 00:00:00\\'), \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'quantity_sold\\': 52}, {\\'transaction_date\\': Timestamp(\\'2024-05-02 00:00:00\\'), \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'quantity_sold\\': 48}, {\\'transaction_date\\': Timestamp(\\'2024-05-02 00:00:00\\'), \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'quantity_sold\\': 55}, {\\'transaction_date\\': Timestamp(\\'2024-05-02 00:00:00\\'), \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'quantity_sold\\': 43}, {\\'transaction_date\\': Timestamp(\\'2024-05-02 00:00:00\\'), \\'store_id\\': \\'S005\\', \\'store_name\\': \\'Southpark Store\\', \\'quantity_sold\\': 50}, {\\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'quantity_sold\\': 48}, {\\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'quantity_sold\\': 45}, {\\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'quantity_sold\\': 50}, {\\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'quantity_sold\\': 38}, {\\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'store_id\\': \\'S005\\', \\'store_name\\': \\'Southpark Store\\', \\'quantity_sold\\': 42}, {\\'transaction_date\\': Timestamp(\\'2024-05-04 00:00:00\\'), \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'quantity_sold\\': 28}, {\\'transaction_date\\': Timestamp(\\'2024-05-04 00:00:00\\'), \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'quantity_sold\\': 15}, {\\'transaction_date\\': Timestamp(\\'2024-05-04 00:00:00\\'), \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'quantity_sold\\': 22}, {\\'transaction_date\\': Timestamp(\\'2024-05-04 00:00:00\\'), \\'store_id\\': \\'S005\\', \\'store_name\\': \\'Southpark Store\\', \\'quantity_sold\\': 18}, {\\'transaction_date\\': Timestamp(\\'2024-05-05 00:00:00\\'), \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'quantity_sold\\': 12}, {\\'transaction_date\\': Timestamp(\\'2024-05-05 00:00:00\\'), \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'quantity_sold\\': 8}, {\\'transaction_date\\': Timestamp(\\'2024-05-05 00:00:00\\'), \\'store_id\\': \\'S005\\', \\'store_name\\': \\'Southpark Store\\', \\'quantity_sold\\': 10}, {\\'transaction_date\\': Timestamp(\\'2024-05-06 00:00:00\\'), \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'quantity_sold\\': 5}, {\\'transaction_date\\': Timestamp(\\'2024-05-06 00:00:00\\'), \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'quantity_sold\\': 3}, {\\'transaction_date\\': Timestamp(\\'2024-05-07 00:00:00\\'), \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'quantity_sold\\': 2}]\", \"tool_call_id\": \"call_wf0s2zTJ0LN7gtjJhlN2VDoY\"}, {\"type\": \"ToolMessage\", \"content\": \"{\\\\\"promo_start\\\\\": \\\\\"2024-05-01\\\\\", \\\\\"promo_end\\\\\": \\\\\"2024-05-07\\\\\"}\", \"tool_call_id\": \"call_jWuUTdXO1n4jWp7RueFIvUKq\"}], \"sales_insights\": \"Sales data shows a significant surge in quantity sold across all stores starting from May 1, 2024, which aligns exactly with the promotional period from May 1 to May 7, 2024. Daily sales before the promotion averaged between 2 and 6 units per store, but during the promotion, these numbers jumped sharply to between 15 and 55 units per store daily. This confirms that the stores experienced unexpectedly high demand driven by promotional activities, supporting the hypotheses that the stockouts are due to demand exceeding forecasted sales and promotional-driven spikes.\"}]}', 'tool_call_id': 'call_dT450NR9Jjaa23y0dl2W0SWg'}, {'type': 'ToolMessage', 'content': '{\"inventory_insights\": {\"demand_vs_stock\": \"Stores experienced high promo sales with quantities sold ranging from 116 to 192 units per store during the promotion period. Initial stock plus receipts theoretically should have covered sales, indicating demand exceeded normal forecasted levels.\", \"replenishment_delays\": \"All stores had replenishment shipments delayed by 2 days during the promo, impacting timely stock availability.\", \"distribution_and_transfers\": \"There was an emergency transfer of 50 units from Northgate Center (S004) to Westside Mall (S002) during the promo, indicating uneven stock distribution and reactive stock rebalancing.\", \"shrinkage_and_adjustments\": \"Westside Mall (S002) reported additional shrinkage of 12 units during the promo period, contributing to stock depletion.\", \"system_and_visibility\": \"No direct evidence from inventory adjustments or transactions indicates system errors, but delayed replenishments and emergency transfers suggest operational strain possibly affecting stock visibility.\", \"supply_chain_disruptions\": \"The repeated delayed replenishments from the central warehouse point to supply chain issues impacting product availability at store level.\", \"summary\": \"Stockouts are primarily driven by unexpectedly high promotional demand, compounded by delayed replenishments and uneven stock distribution requiring emergency transfers. Additional shrinkage further exacerbates shortages at some stores.\"}, \"trace\": [{\"agent\": \"InventoryAnalysisAgent\", \"step\": \"Validated inventory hypotheses\", \"calls\": [{\"type\": \"AIMessage\", \"content\": \"\", \"tool_calls\": [{\"name\": \"get_promo_period\", \"args\": {}, \"id\": \"call_qFkVGl4NP1xpeiCege6CQAk4\"}, {\"name\": \"get_unique_stores\", \"args\": {}, \"id\": \"call_4VH9tvkj7MYzST5cPeeNPsKK\"}]}, {\"type\": \"ToolMessage\", \"content\": \"{\\\\\"promo_start\\\\\": \\\\\"2024-05-01\\\\\", \\\\\"promo_end\\\\\": \\\\\"2024-05-07\\\\\"}\", \"tool_call_id\": \"call_qFkVGl4NP1xpeiCege6CQAk4\"}, {\"type\": \"ToolMessage\", \"content\": \"{\\\\\"stores\\\\\": [\\\\\"S001\\\\\", \\\\\"S002\\\\\", \\\\\"S003\\\\\", \\\\\"S004\\\\\", \\\\\"S005\\\\\", \\\\\"WAREHOUSE_CENTRAL\\\\\"]}\", \"tool_call_id\": \"call_4VH9tvkj7MYzST5cPeeNPsKK\"}, {\"type\": \"AIMessage\", \"content\": \"\", \"tool_calls\": [{\"name\": \"theoretical_onhand_after_promo_sales\", \"args\": {\"promo_start\": \"2024-05-01\", \"promo_end\": \"2024-05-07\"}, \"id\": \"call_zlZqOf5Ov4JEWMzI1c3DCBui\"}, {\"name\": \"get_delayed_replenishments\", \"args\": {}, \"id\": \"call_sYOjgQQOLDGGRdOHqepklRqV\"}, {\"name\": \"get_shrinkage_during_promo\", \"args\": {\"promo_start\": \"2024-05-01\", \"promo_end\": \"2024-05-07\"}, \"id\": \"call_lW5r5cmAudraPDweJErE8X8g\"}, {\"name\": \"get_all_transfers\", \"args\": {}, \"id\": \"call_SPOg2ITT8H23ZlsOCGBtrrlz\"}]}, {\"type\": \"ToolMessage\", \"content\": \"[{\\\\\"store_id\\\\\": \\\\\"S001\\\\\", \\\\\"store_name\\\\\": \\\\\"Downtown Store\\\\\", \\\\\"start_receipt_qty\\\\\": 300, \\\\\"net_qty_after\\\\\": 250, \\\\\"promo_qty_sold\\\\\": 192, \\\\\"promo_repl_qty\\\\\": 0.0, \\\\\"theoretical_after_changes\\\\\": 550, \\\\\"theoretical_onhand_after_promo_sales\\\\\": 358}, {\\\\\"store_id\\\\\": \\\\\"S002\\\\\", \\\\\"store_name\\\\\": \\\\\"Westside Mall\\\\\", \\\\\"start_receipt_qty\\\\\": 250, \\\\\"net_qty_after\\\\\": 198, \\\\\"promo_qty_sold\\\\\": 146, \\\\\"promo_repl_qty\\\\\": 0.0, \\\\\"theoretical_after_changes\\\\\": 448, \\\\\"theoretical_onhand_after_promo_sales\\\\\": 302}, {\\\\\"store_id\\\\\": \\\\\"S003\\\\\", \\\\\"store_name\\\\\": \\\\\"Eastview Plaza\\\\\", \\\\\"start_receipt_qty\\\\\": 280, \\\\\"net_qty_after\\\\\": 250, \\\\\"promo_qty_sold\\\\\": 180, \\\\\"promo_repl_qty\\\\\": 0.0, \\\\\"theoretical_after_changes\\\\\": 530, \\\\\"theoretical_onhand_after_promo_sales\\\\\": 350}, {\\\\\"store_id\\\\\": \\\\\"S004\\\\\", \\\\\"store_name\\\\\": \\\\\"Northgate Center\\\\\", \\\\\"start_receipt_qty\\\\\": 220, \\\\\"net_qty_after\\\\\": 180, \\\\\"promo_qty_sold\\\\\": 116, \\\\\"promo_repl_qty\\\\\": 0.0, \\\\\"theoretical_after_changes\\\\\": 400, \\\\\"theoretical_onhand_after_promo_sales\\\\\": 284}, {\\\\\"store_id\\\\\": \\\\\"S005\\\\\", \\\\\"store_name\\\\\": \\\\\"Southpark Store\\\\\", \\\\\"start_receipt_qty\\\\\": 260, \\\\\"net_qty_after\\\\\": 225, \\\\\"promo_qty_sold\\\\\": 160, \\\\\"promo_repl_qty\\\\\": 0.0, \\\\\"theoretical_after_changes\\\\\": 485, \\\\\"theoretical_onhand_after_promo_sales\\\\\": 325}]\", \"tool_call_id\": \"call_zlZqOf5Ov4JEWMzI1c3DCBui\"}, {\"type\": \"ToolMessage\", \"content\": \"[{\\'transaction_id\\': \\'INV013\\', \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'transaction_type\\': \\'RECEIPT\\', \\'quantity\\': 150, \\'source_location\\': \\'WAREHOUSE_CENTRAL\\', \\'destination_location\\': \\'S001\\', \\'notes\\': \\'Promotion replenishment - DELAYED 2 days\\'}, {\\'transaction_id\\': \\'INV014\\', \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'transaction_type\\': \\'RECEIPT\\', \\'quantity\\': 120, \\'source_location\\': \\'WAREHOUSE_CENTRAL\\', \\'destination_location\\': \\'S002\\', \\'notes\\': \\'Promotion replenishment - DELAYED 2 days\\'}, {\\'transaction_id\\': \\'INV015\\', \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'transaction_type\\': \\'RECEIPT\\', \\'quantity\\': 140, \\'source_location\\': \\'WAREHOUSE_CENTRAL\\', \\'destination_location\\': \\'S003\\', \\'notes\\': \\'Promotion replenishment - DELAYED 2 days\\'}, {\\'transaction_id\\': \\'INV016\\', \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'transaction_type\\': \\'RECEIPT\\', \\'quantity\\': 100, \\'source_location\\': \\'WAREHOUSE_CENTRAL\\', \\'destination_location\\': \\'S004\\', \\'notes\\': \\'Promotion replenishment - DELAYED 2 days\\'}, {\\'transaction_id\\': \\'INV017\\', \\'store_id\\': \\'S005\\', \\'store_name\\': \\'Southpark Store\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'transaction_type\\': \\'RECEIPT\\', \\'quantity\\': 130, \\'source_location\\': \\'WAREHOUSE_CENTRAL\\', \\'destination_location\\': \\'S005\\', \\'notes\\': \\'Promotion replenishment - DELAYED 2 days\\'}]\", \"tool_call_id\": \"call_sYOjgQQOLDGGRdOHqepklRqV\"}, {\"type\": \"ToolMessage\", \"content\": \"[{\\'transaction_id\\': \\'INV018\\', \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-04 00:00:00\\'), \\'transaction_type\\': \\'ADJUSTMENT\\', \\'quantity\\': -12, \\'source_location\\': \\'S002\\', \\'destination_location\\': \\'NONE\\', \\'notes\\': \\'Additional shrinkage found during stockout investigation\\'}]\", \"tool_call_id\": \"call_lW5r5cmAudraPDweJErE8X8g\"}, {\"type\": \"ToolMessage\", \"content\": \"[{\\'transaction_id\\': \\'INV011\\', \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-01 00:00:00\\'), \\'transaction_type\\': \\'TRANSFER\\', \\'quantity\\': -50, \\'source_location\\': \\'S004\\', \\'destination_location\\': \\'S002\\', \\'notes\\': \\'Emergency transfer to Westside Mall\\'}, {\\'transaction_id\\': \\'INV012\\', \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-01 00:00:00\\'), \\'transaction_type\\': \\'TRANSFER\\', \\'quantity\\': 50, \\'source_location\\': \\'S004\\', \\'destination_location\\': \\'S002\\', \\'notes\\': \\'Emergency transfer from Northgate\\'}]\", \"tool_call_id\": \"call_SPOg2ITT8H23ZlsOCGBtrrlz\"}], \"inventory_insights\": {\"demand_vs_stock\": \"Stores experienced high promo sales with quantities sold ranging from 116 to 192 units per store during the promotion period. Initial stock plus receipts theoretically should have covered sales, indicating demand exceeded normal forecasted levels.\", \"replenishment_delays\": \"All stores had replenishment shipments delayed by 2 days during the promo, impacting timely stock availability.\", \"distribution_and_transfers\": \"There was an emergency transfer of 50 units from Northgate Center (S004) to Westside Mall (S002) during the promo, indicating uneven stock distribution and reactive stock rebalancing.\", \"shrinkage_and_adjustments\": \"Westside Mall (S002) reported additional shrinkage of 12 units during the promo period, contributing to stock depletion.\", \"system_and_visibility\": \"No direct evidence from inventory adjustments or transactions indicates system errors, but delayed replenishments and emergency transfers suggest operational strain possibly affecting stock visibility.\", \"supply_chain_disruptions\": \"The repeated delayed replenishments from the central warehouse point to supply chain issues impacting product availability at store level.\", \"summary\": \"Stockouts are primarily driven by unexpectedly high promotional demand, compounded by delayed replenishments and uneven stock distribution requiring emergency transfers. Additional shrinkage further exacerbates shortages at some stores.\"}}]}', 'tool_call_id': 'call_Upr3rou4yHyYNpgpXS3uiftW'}, {'type': 'AIMessage', 'content': '', 'tool_calls': [{'name': 'hypothesis_validation_agent_tool', 'args': {'hypotheses': ['Unexpectedly high demand exceeding forecasted sales causing depletion of stock.', 'Delays or disruptions in inventory replenishment or shipments leading to stock shortages.', 'Inefficient distribution or allocation causing uneven stock levels across different stores.', 'Errors or inaccuracies in point-of-sale or inventory management systems resulting in poor stock visibility.', 'Supply chain disruptions affecting the availability of products.', 'Promotional activities or pricing strategies driving sudden spikes in demand.', 'Stock mismanagement or shrinkage such as theft, damage, or administrative errors at store level.'], 'user_id': '2', 'query_id': '2'}, 'id': 'call_3NXy7gdVasb9HOZt7ofvamd5'}]}, {'type': 'ToolMessage', 'content': \"Error invoking tool 'hypothesis_validation_agent_tool' with kwargs {'hypotheses': ['Unexpectedly high demand exceeding forecasted sales causing depletion of stock.', 'Delays or disruptions in inventory replenishment or shipments leading to stock shortages.', 'Inefficient distribution or allocation causing uneven stock levels across different stores.', 'Errors or inaccuracies in point-of-sale or inventory management systems resulting in poor stock visibility.', 'Supply chain disruptions affecting the availability of products.', 'Promotional activities or pricing strategies driving sudden spikes in demand.', 'Stock mismanagement or shrinkage such as theft, damage, or administrative errors at store level.'], 'user_id': '2', 'query_id': '2'} with error:\\n sales_insights: Field required\\ninventory_insights: Field required\\n Please fix the error and try again.\", 'tool_call_id': 'call_3NXy7gdVasb9HOZt7ofvamd5'}, {'type': 'AIMessage', 'content': '', 'tool_calls': [{'name': 'hypothesis_validation_agent_tool', 'args': {'hypotheses': ['Unexpectedly high demand exceeding forecasted sales causing depletion of stock.', 'Delays or disruptions in inventory replenishment or shipments leading to stock shortages.', 'Inefficient distribution or allocation causing uneven stock levels across different stores.', 'Errors or inaccuracies in point-of-sale or inventory management systems resulting in poor stock visibility.', 'Supply chain disruptions affecting the availability of products.', 'Promotional activities or pricing strategies driving sudden spikes in demand.', 'Stock mismanagement or shrinkage such as theft, damage, or administrative errors at store level.'], 'sales_insights': {'sales_insights': 'Sales data shows a significant surge in quantity sold across all stores starting from May 1, 2024, which aligns exactly with the promotional period from May 1 to May 7, 2024. Daily sales before the promotion averaged between 2 and 6 units per store, but during the promotion, these numbers jumped sharply to between 15 and 55 units per store daily. This confirms that the stores experienced unexpectedly high demand driven by promotional activities, supporting the hypotheses that the stockouts are due to demand exceeding forecasted sales and promotional-driven spikes.'}, 'inventory_insights': {'demand_vs_stock': 'Stores experienced high promo sales with quantities sold ranging from 116 to 192 units per store during the promotion period. Initial stock plus receipts theoretically should have covered sales, indicating demand exceeded normal forecasted levels.', 'replenishment_delays': 'All stores had replenishment shipments delayed by 2 days during the promo, impacting timely stock availability.', 'distribution_and_transfers': 'There was an emergency transfer of 50 units from Northgate Center (S004) to Westside Mall (S002) during the promo, indicating uneven stock distribution and reactive stock rebalancing.', 'shrinkage_and_adjustments': 'Westside Mall (S002) reported additional shrinkage of 12 units during the promo period, contributing to stock depletion.', 'system_and_visibility': 'No direct evidence from inventory adjustments or transactions indicates system errors, but delayed replenishments and emergency transfers suggest operational strain possibly affecting stock visibility.', 'supply_chain_disruptions': 'The repeated delayed replenishments from the central warehouse point to supply chain issues impacting product availability at store level.', 'summary': 'Stockouts are primarily driven by unexpectedly high promotional demand, compounded by delayed replenishments and uneven stock distribution requiring emergency transfers. Additional shrinkage further exacerbates shortages at some stores.'}, 'user_id': '2', 'query_id': '2'}, 'id': 'call_tGfqMkC4z4Bg62zTI9Zqq61O'}]}, {'type': 'ToolMessage', 'content': '{\"validated\": {\"Unexpectedly high demand exceeding forecasted sales causing depletion of stock.\": true, \"Delays or disruptions in inventory replenishment or shipments leading to stock shortages.\": true, \"Inefficient distribution or allocation causing uneven stock levels across different stores.\": true, \"Errors or inaccuracies in point-of-sale or inventory management systems resulting in poor stock visibility.\": false, \"Supply chain disruptions affecting the availability of products.\": true, \"Promotional activities or pricing strategies driving sudden spikes in demand.\": true, \"Stock mismanagement or shrinkage such as theft, damage, or administrative errors at store level.\": true}, \"reasoning\": {\"Unexpectedly high demand exceeding forecasted sales causing depletion of stock.\": \"Sales data confirms a significant surge in demand during the promotional period with quantities sold far exceeding normal levels.\", \"Delays or disruptions in inventory replenishment or shipments leading to stock shortages.\": \"Inventory data shows all stores experienced a 2-day delay in replenishment shipments, affecting stock availability.\", \"Inefficient distribution or allocation causing uneven stock levels across different stores.\": \"Emergency transfer of stock between stores during the promo indicates uneven initial stock allocation requiring reactive adjustments.\", \"Errors or inaccuracies in point-of-sale or inventory management systems resulting in poor stock visibility.\": \"No direct evidence of system errors or inaccuracies was found in inventory adjustments or transactions, though operational strain was noted.\", \"Supply chain disruptions affecting the availability of products.\": \"Repeated delayed shipments from the central warehouse suggest supply chain issues impacting store-level availability.\", \"Promotional activities or pricing strategies driving sudden spikes in demand.\": \"Sales surge aligns exactly with the promotional period, confirming promotional activities as a key driver of demand spikes.\", \"Stock mismanagement or shrinkage such as theft, damage, or administrative errors at store level.\": \"Shrinkage adjustments were recorded at one store during the promo, indicating stock loss contributing to shortages.\"}, \"trace\": [{\"agent\": \"HypothesisValidationAgent\", \"step\": \"Validated hypotheses\", \"calls\": [{\"type\": \"AIMessage\", \"content\": \"\", \"tool_calls\": [{\"name\": \"get_promo_period\", \"args\": {}, \"id\": \"call_qFkVGl4NP1xpeiCege6CQAk4\"}, {\"name\": \"get_unique_stores\", \"args\": {}, \"id\": \"call_4VH9tvkj7MYzST5cPeeNPsKK\"}]}, {\"type\": \"ToolMessage\", \"content\": \"{\\\\\"promo_start\\\\\": \\\\\"2024-05-01\\\\\", \\\\\"promo_end\\\\\": \\\\\"2024-05-07\\\\\"}\", \"tool_call_id\": \"call_qFkVGl4NP1xpeiCege6CQAk4\"}, {\"type\": \"ToolMessage\", \"content\": \"{\\\\\"stores\\\\\": [\\\\\"S001\\\\\", \\\\\"S002\\\\\", \\\\\"S003\\\\\", \\\\\"S004\\\\\", \\\\\"S005\\\\\", \\\\\"WAREHOUSE_CENTRAL\\\\\"]}\", \"tool_call_id\": \"call_4VH9tvkj7MYzST5cPeeNPsKK\"}, {\"type\": \"AIMessage\", \"content\": \"\", \"tool_calls\": [{\"name\": \"theoretical_onhand_after_promo_sales\", \"args\": {\"promo_start\": \"2024-05-01\", \"promo_end\": \"2024-05-07\"}, \"id\": \"call_zlZqOf5Ov4JEWMzI1c3DCBui\"}, {\"name\": \"get_delayed_replenishments\", \"args\": {}, \"id\": \"call_sYOjgQQOLDGGRdOHqepklRqV\"}, {\"name\": \"get_shrinkage_during_promo\", \"args\": {\"promo_start\": \"2024-05-01\", \"promo_end\": \"2024-05-07\"}, \"id\": \"call_lW5r5cmAudraPDweJErE8X8g\"}, {\"name\": \"get_all_transfers\", \"args\": {}, \"id\": \"call_SPOg2ITT8H23ZlsOCGBtrrlz\"}]}, {\"type\": \"ToolMessage\", \"content\": \"[{\\\\\"store_id\\\\\": \\\\\"S001\\\\\", \\\\\"store_name\\\\\": \\\\\"Downtown Store\\\\\", \\\\\"start_receipt_qty\\\\\": 300, \\\\\"net_qty_after\\\\\": 250, \\\\\"promo_qty_sold\\\\\": 192, \\\\\"promo_repl_qty\\\\\": 0.0, \\\\\"theoretical_after_changes\\\\\": 550, \\\\\"theoretical_onhand_after_promo_sales\\\\\": 358}, {\\\\\"store_id\\\\\": \\\\\"S002\\\\\", \\\\\"store_name\\\\\": \\\\\"Westside Mall\\\\\", \\\\\"start_receipt_qty\\\\\": 250, \\\\\"net_qty_after\\\\\": 198, \\\\\"promo_qty_sold\\\\\": 146, \\\\\"promo_repl_qty\\\\\": 0.0, \\\\\"theoretical_after_changes\\\\\": 448, \\\\\"theoretical_onhand_after_promo_sales\\\\\": 302}, {\\\\\"store_id\\\\\": \\\\\"S003\\\\\", \\\\\"store_name\\\\\": \\\\\"Eastview Plaza\\\\\", \\\\\"start_receipt_qty\\\\\": 280, \\\\\"net_qty_after\\\\\": 250, \\\\\"promo_qty_sold\\\\\": 180, \\\\\"promo_repl_qty\\\\\": 0.0, \\\\\"theoretical_after_changes\\\\\": 530, \\\\\"theoretical_onhand_after_promo_sales\\\\\": 350}, {\\\\\"store_id\\\\\": \\\\\"S004\\\\\", \\\\\"store_name\\\\\": \\\\\"Northgate Center\\\\\", \\\\\"start_receipt_qty\\\\\": 220, \\\\\"net_qty_after\\\\\": 180, \\\\\"promo_qty_sold\\\\\": 116, \\\\\"promo_repl_qty\\\\\": 0.0, \\\\\"theoretical_after_changes\\\\\": 400, \\\\\"theoretical_onhand_after_promo_sales\\\\\": 284}, {\\\\\"store_id\\\\\": \\\\\"S005\\\\\", \\\\\"store_name\\\\\": \\\\\"Southpark Store\\\\\", \\\\\"start_receipt_qty\\\\\": 260, \\\\\"net_qty_after\\\\\": 225, \\\\\"promo_qty_sold\\\\\": 160, \\\\\"promo_repl_qty\\\\\": 0.0, \\\\\"theoretical_after_changes\\\\\": 485, \\\\\"theoretical_onhand_after_promo_sales\\\\\": 325}]\", \"tool_call_id\": \"call_zlZqOf5Ov4JEWMzI1c3DCBui\"}, {\"type\": \"ToolMessage\", \"content\": \"[{\\'transaction_id\\': \\'INV013\\', \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'transaction_type\\': \\'RECEIPT\\', \\'quantity\\': 150, \\'source_location\\': \\'WAREHOUSE_CENTRAL\\', \\'destination_location\\': \\'S001\\', \\'notes\\': \\'Promotion replenishment - DELAYED 2 days\\'}, {\\'transaction_id\\': \\'INV014\\', \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'transaction_type\\': \\'RECEIPT\\', \\'quantity\\': 120, \\'source_location\\': \\'WAREHOUSE_CENTRAL\\', \\'destination_location\\': \\'S002\\', \\'notes\\': \\'Promotion replenishment - DELAYED 2 days\\'}, {\\'transaction_id\\': \\'INV015\\', \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'transaction_type\\': \\'RECEIPT\\', \\'quantity\\': 140, \\'source_location\\': \\'WAREHOUSE_CENTRAL\\', \\'destination_location\\': \\'S003\\', \\'notes\\': \\'Promotion replenishment - DELAYED 2 days\\'}, {\\'transaction_id\\': \\'INV016\\', \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'transaction_type\\': \\'RECEIPT\\', \\'quantity\\': 100, \\'source_location\\': \\'WAREHOUSE_CENTRAL\\', \\'destination_location\\': \\'S004\\', \\'notes\\': \\'Promotion replenishment - DELAYED 2 days\\'}, {\\'transaction_id\\': \\'INV017\\', \\'store_id\\': \\'S005\\', \\'store_name\\': \\'Southpark Store\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'transaction_type\\': \\'RECEIPT\\', \\'quantity\\': 130, \\'source_location\\': \\'WAREHOUSE_CENTRAL\\', \\'destination_location\\': \\'S005\\', \\'notes\\': \\'Promotion replenishment - DELAYED 2 days\\'}]\", \"tool_call_id\": \"call_sYOjgQQOLDGGRdOHqepklRqV\"}, {\"type\": \"ToolMessage\", \"content\": \"[{\\'transaction_id\\': \\'INV018\\', \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-04 00:00:00\\'), \\'transaction_type\\': \\'ADJUSTMENT\\', \\'quantity\\': -12, \\'source_location\\': \\'S002\\', \\'destination_location\\': \\'NONE\\', \\'notes\\': \\'Additional shrinkage found during stockout investigation\\'}]\", \"tool_call_id\": \"call_lW5r5cmAudraPDweJErE8X8g\"}, {\"type\": \"ToolMessage\", \"content\": \"[{\\'transaction_id\\': \\'INV011\\', \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-01 00:00:00\\'), \\'transaction_type\\': \\'TRANSFER\\', \\'quantity\\': -50, \\'source_location\\': \\'S004\\', \\'destination_location\\': \\'S002\\', \\'notes\\': \\'Emergency transfer to Westside Mall\\'}, {\\'transaction_id\\': \\'INV012\\', \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-01 00:00:00\\'), \\'transaction_type\\': \\'TRANSFER\\', \\'quantity\\': 50, \\'source_location\\': \\'S004\\', \\'destination_location\\': \\'S002\\', \\'notes\\': \\'Emergency transfer from Northgate\\'}]\", \"tool_call_id\": \"call_SPOg2ITT8H23ZlsOCGBtrrlz\"}], \"details\": {\"validated\": {\"Unexpectedly high demand exceeding forecasted sales causing depletion of stock.\": true, \"Delays or disruptions in inventory replenishment or shipments leading to stock shortages.\": true, \"Inefficient distribution or allocation causing uneven stock levels across different stores.\": true, \"Errors or inaccuracies in point-of-sale or inventory management systems resulting in poor stock visibility.\": false, \"Supply chain disruptions affecting the availability of products.\": true, \"Promotional activities or pricing strategies driving sudden spikes in demand.\": true, \"Stock mismanagement or shrinkage such as theft, damage, or administrative errors at store level.\": true}, \"reasoning\": {\"Unexpectedly high demand exceeding forecasted sales causing depletion of stock.\": \"Sales data confirms a significant surge in demand during the promotional period with quantities sold far exceeding normal levels.\", \"Delays or disruptions in inventory replenishment or shipments leading to stock shortages.\": \"Inventory data shows all stores experienced a 2-day delay in replenishment shipments, affecting stock availability.\", \"Inefficient distribution or allocation causing uneven stock levels across different stores.\": \"Emergency transfer of stock between stores during the promo indicates uneven initial stock allocation requiring reactive adjustments.\", \"Errors or inaccuracies in point-of-sale or inventory management systems resulting in poor stock visibility.\": \"No direct evidence of system errors or inaccuracies was found in inventory adjustments or transactions, though operational strain was noted.\", \"Supply chain disruptions affecting the availability of products.\": \"Repeated delayed shipments from the central warehouse suggest supply chain issues impacting store-level availability.\", \"Promotional activities or pricing strategies driving sudden spikes in demand.\": \"Sales surge aligns exactly with the promotional period, confirming promotional activities as a key driver of demand spikes.\", \"Stock mismanagement or shrinkage such as theft, damage, or administrative errors at store level.\": \"Shrinkage adjustments were recorded at one store during the promo, indicating stock loss contributing to shortages.\"}}}]}', 'tool_call_id': 'call_tGfqMkC4z4Bg62zTI9Zqq61O'}, {'type': 'AIMessage', 'content': '', 'tool_calls': [{'name': 'root_cause_analysis_agent_tool', 'args': {'user_id': '2', 'query_id': '2'}, 'id': 'call_JgUA3SQMiWgAYVaSMYPg6whF'}]}, {'type': 'ToolMessage', 'content': \"Error invoking tool 'root_cause_analysis_agent_tool' with kwargs {'user_id': '2', 'query_id': '2'} with error:\\n validated_hypotheses: Field required\\nsales_insights: Field required\\ninventory_insights: Field required\\ntrace: Field required\\n Please fix the error and try again.\", 'tool_call_id': 'call_JgUA3SQMiWgAYVaSMYPg6whF'}, {'type': 'AIMessage', 'content': '', 'tool_calls': [{'name': 'root_cause_analysis_agent_tool', 'args': {'user_id': '2', 'query_id': '2', 'validated_hypotheses': {'Unexpectedly high demand exceeding forecasted sales causing depletion of stock.': True, 'Delays or disruptions in inventory replenishment or shipments leading to stock shortages.': True, 'Inefficient distribution or allocation causing uneven stock levels across different stores.': True, 'Errors or inaccuracies in point-of-sale or inventory management systems resulting in poor stock visibility.': False, 'Supply chain disruptions affecting the availability of products.': True, 'Promotional activities or pricing strategies driving sudden spikes in demand.': True, 'Stock mismanagement or shrinkage such as theft, damage, or administrative errors at store level.': True}, 'sales_insights': {'sales_insights': 'Sales data shows a significant surge in quantity sold across all stores starting from May 1, 2024, which aligns exactly with the promotional period from May 1 to May 7, 2024. Daily sales before the promotion averaged between 2 and 6 units per store, but during the promotion, these numbers jumped sharply to between 15 and 55 units per store daily. This confirms that the stores experienced unexpectedly high demand driven by promotional activities, supporting the hypotheses that the stockouts are due to demand exceeding forecasted sales and promotional-driven spikes.'}, 'inventory_insights': {'demand_vs_stock': 'Stores experienced high promo sales with quantities sold ranging from 116 to 192 units per store during the promotion period. Initial stock plus receipts theoretically should have covered sales, indicating demand exceeded normal forecasted levels.', 'replenishment_delays': 'All stores had replenishment shipments delayed by 2 days during the promo, impacting timely stock availability.', 'distribution_and_transfers': 'There was an emergency transfer of 50 units from Northgate Center (S004) to Westside Mall (S002) during the promo, indicating uneven stock distribution and reactive stock rebalancing.', 'shrinkage_and_adjustments': 'Westside Mall (S002) reported additional shrinkage of 12 units during the promo period, contributing to stock depletion.', 'system_and_visibility': 'No direct evidence from inventory adjustments or transactions indicates system errors, but delayed replenishments and emergency transfers suggest operational strain possibly affecting stock visibility.', 'supply_chain_disruptions': 'The repeated delayed replenishments from the central warehouse point to supply chain issues impacting product availability at store level.', 'summary': 'Stockouts are primarily driven by unexpectedly high promotional demand, compounded by delayed replenishments and uneven stock distribution requiring emergency transfers. Additional shrinkage further exacerbates shortages at some stores.'}, 'trace': []}, 'id': 'call_jI3Gt11tvTS9m5GLVjIpNIGZ'}]}, {'type': 'ToolMessage', 'content': '{\"root_cause\": {\"primary_root_causes\": [\"Unexpectedly high demand exceeding forecasted sales driven by promotional activities.\", \"Delays and disruptions in inventory replenishment and shipments causing stock shortages.\", \"Inefficient distribution and allocation leading to uneven stock levels across stores.\", \"Supply chain disruptions affecting timely product availability.\"], \"supporting_evidence\": {\"sales\": {\"promotion_period\": \"May 1 to May 7, 2024\", \"demand_surge\": \"Daily sales increased from 2-6 units pre-promo to 15-55 units during promo across all stores.\"}, \"inventory\": {\"high_sales_volume\": \"Stores sold 116 to 192 units during promo, exceeding normal forecast.\", \"replenishment_delay\": \"All stores had replenishment shipments delayed by 2 days during promo.\", \"emergency_transfers\": \"50 units transferred from Northgate Center to Westside Mall indicating uneven distribution.\", \"shrinkage\": \"12 units shrinkage recorded at Westside Mall during promo.\"}, \"cross_analysis\": {\"stockouts_cause\": \"High demand combined with replenishment delays and uneven stock allocation led to stockouts.\", \"supply_chain_impact\": \"Delayed shipments from central warehouse indicate supply chain issues impacting store availability.\"}}, \"contributing_factors\": [\"Promotional pricing and marketing driving sudden demand spikes.\", \"Stock mismanagement and shrinkage at store level exacerbating shortages.\", \"Operational strain possibly affecting inventory visibility due to delayed replenishments and emergency transfers.\"], \"timeline\": [{\"date\": \"2024-05-01\", \"event\": \"Promotion started, daily sales surged significantly across all stores.\"}, {\"date\": \"2024-05-01\", \"event\": \"Emergency stock transfer of 50 units from Northgate Center to Westside Mall to address uneven stock.\"}, {\"date\": \"2024-05-03\", \"event\": \"Replenishment shipments from central warehouse delayed by 2 days for all stores.\"}, {\"date\": \"2024-05-04\", \"event\": \"Shrinkage adjustment of 12 units recorded at Westside Mall.\"}, {\"date\": \"2024-05-07\", \"event\": \"Promotion ended.\"}], \"recommendations\": [\"Improve demand forecasting models to better anticipate promotional demand spikes.\", \"Enhance supply chain processes to reduce replenishment delays, including contingency plans for promotions.\", \"Optimize stock allocation and distribution to balance inventory across stores ahead of promotions.\", \"Implement tighter controls and monitoring to reduce shrinkage during high-demand periods.\", \"Strengthen inventory visibility and operational coordination during promotions to proactively manage stock levels.\"]}, \"reasoning\": {\"primary_root_causes\": \"The analysis clearly shows that unexpectedly high promotional demand exceeded forecasts, stressing inventory. Delays in replenishment shipments and uneven stock distribution further contributed to stock shortages. Supply chain disruptions from central warehouse shipments delayed timely stock availability.\", \"contributing_factors\": \"Promotional activities caused demand spikes. Stock shrinkage at store level and operational strain during replenishment and emergency transfers worsened stockouts.\", \"supporting_evidence\": \"Sales data demonstrated a sharp increase in demand during the promotion. Inventory data confirmed replenishment delays, emergency transfers, and shrinkage incidents, collectively supporting the root causes.\", \"timeline\": \"The timeline highlights key events starting with the promotion launch and demand surge, followed by emergency stock transfers and replenishment delays, concluding with shrinkage findings and promotion end.\", \"recommendations\": \"Recommendations focus on improving demand forecasting, supply chain reliability, stock distribution, shrinkage control, and inventory visibility to prevent recurrence of similar stockouts.\"}, \"trace\": [{\"agent\": \"RootCauseAnalysisAgent\", \"step\": \"Generated structured root cause\", \"calls\": [{\"type\": \"AIMessage\", \"content\": \"\", \"tool_calls\": [{\"name\": \"get_promo_period\", \"args\": {}, \"id\": \"call_qFkVGl4NP1xpeiCege6CQAk4\"}, {\"name\": \"get_unique_stores\", \"args\": {}, \"id\": \"call_4VH9tvkj7MYzST5cPeeNPsKK\"}]}, {\"type\": \"ToolMessage\", \"content\": \"{\\\\\"promo_start\\\\\": \\\\\"2024-05-01\\\\\", \\\\\"promo_end\\\\\": \\\\\"2024-05-07\\\\\"}\", \"tool_call_id\": \"call_qFkVGl4NP1xpeiCege6CQAk4\"}, {\"type\": \"ToolMessage\", \"content\": \"{\\\\\"stores\\\\\": [\\\\\"S001\\\\\", \\\\\"S002\\\\\", \\\\\"S003\\\\\", \\\\\"S004\\\\\", \\\\\"S005\\\\\", \\\\\"WAREHOUSE_CENTRAL\\\\\"]}\", \"tool_call_id\": \"call_4VH9tvkj7MYzST5cPeeNPsKK\"}, {\"type\": \"AIMessage\", \"content\": \"\", \"tool_calls\": [{\"name\": \"theoretical_onhand_after_promo_sales\", \"args\": {\"promo_start\": \"2024-05-01\", \"promo_end\": \"2024-05-07\"}, \"id\": \"call_zlZqOf5Ov4JEWMzI1c3DCBui\"}, {\"name\": \"get_delayed_replenishments\", \"args\": {}, \"id\": \"call_sYOjgQQOLDGGRdOHqepklRqV\"}, {\"name\": \"get_shrinkage_during_promo\", \"args\": {\"promo_start\": \"2024-05-01\", \"promo_end\": \"2024-05-07\"}, \"id\": \"call_lW5r5cmAudraPDweJErE8X8g\"}, {\"name\": \"get_all_transfers\", \"args\": {}, \"id\": \"call_SPOg2ITT8H23ZlsOCGBtrrlz\"}]}, {\"type\": \"ToolMessage\", \"content\": \"[{\\\\\"store_id\\\\\": \\\\\"S001\\\\\", \\\\\"store_name\\\\\": \\\\\"Downtown Store\\\\\", \\\\\"start_receipt_qty\\\\\": 300, \\\\\"net_qty_after\\\\\": 250, \\\\\"promo_qty_sold\\\\\": 192, \\\\\"promo_repl_qty\\\\\": 0.0, \\\\\"theoretical_after_changes\\\\\": 550, \\\\\"theoretical_onhand_after_promo_sales\\\\\": 358}, {\\\\\"store_id\\\\\": \\\\\"S002\\\\\", \\\\\"store_name\\\\\": \\\\\"Westside Mall\\\\\", \\\\\"start_receipt_qty\\\\\": 250, \\\\\"net_qty_after\\\\\": 198, \\\\\"promo_qty_sold\\\\\": 146, \\\\\"promo_repl_qty\\\\\": 0.0, \\\\\"theoretical_after_changes\\\\\": 448, \\\\\"theoretical_onhand_after_promo_sales\\\\\": 302}, {\\\\\"store_id\\\\\": \\\\\"S003\\\\\", \\\\\"store_name\\\\\": \\\\\"Eastview Plaza\\\\\", \\\\\"start_receipt_qty\\\\\": 280, \\\\\"net_qty_after\\\\\": 250, \\\\\"promo_qty_sold\\\\\": 180, \\\\\"promo_repl_qty\\\\\": 0.0, \\\\\"theoretical_after_changes\\\\\": 530, \\\\\"theoretical_onhand_after_promo_sales\\\\\": 350}, {\\\\\"store_id\\\\\": \\\\\"S004\\\\\", \\\\\"store_name\\\\\": \\\\\"Northgate Center\\\\\", \\\\\"start_receipt_qty\\\\\": 220, \\\\\"net_qty_after\\\\\": 180, \\\\\"promo_qty_sold\\\\\": 116, \\\\\"promo_repl_qty\\\\\": 0.0, \\\\\"theoretical_after_changes\\\\\": 400, \\\\\"theoretical_onhand_after_promo_sales\\\\\": 284}, {\\\\\"store_id\\\\\": \\\\\"S005\\\\\", \\\\\"store_name\\\\\": \\\\\"Southpark Store\\\\\", \\\\\"start_receipt_qty\\\\\": 260, \\\\\"net_qty_after\\\\\": 225, \\\\\"promo_qty_sold\\\\\": 160, \\\\\"promo_repl_qty\\\\\": 0.0, \\\\\"theoretical_after_changes\\\\\": 485, \\\\\"theoretical_onhand_after_promo_sales\\\\\": 325}]\", \"tool_call_id\": \"call_zlZqOf5Ov4JEWMzI1c3DCBui\"}, {\"type\": \"ToolMessage\", \"content\": \"[{\\'transaction_id\\': \\'INV013\\', \\'store_id\\': \\'S001\\', \\'store_name\\': \\'Downtown Store\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'transaction_type\\': \\'RECEIPT\\', \\'quantity\\': 150, \\'source_location\\': \\'WAREHOUSE_CENTRAL\\', \\'destination_location\\': \\'S001\\', \\'notes\\': \\'Promotion replenishment - DELAYED 2 days\\'}, {\\'transaction_id\\': \\'INV014\\', \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'transaction_type\\': \\'RECEIPT\\', \\'quantity\\': 120, \\'source_location\\': \\'WAREHOUSE_CENTRAL\\', \\'destination_location\\': \\'S002\\', \\'notes\\': \\'Promotion replenishment - DELAYED 2 days\\'}, {\\'transaction_id\\': \\'INV015\\', \\'store_id\\': \\'S003\\', \\'store_name\\': \\'Eastview Plaza\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'transaction_type\\': \\'RECEIPT\\', \\'quantity\\': 140, \\'source_location\\': \\'WAREHOUSE_CENTRAL\\', \\'destination_location\\': \\'S003\\', \\'notes\\': \\'Promotion replenishment - DELAYED 2 days\\'}, {\\'transaction_id\\': \\'INV016\\', \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'transaction_type\\': \\'RECEIPT\\', \\'quantity\\': 100, \\'source_location\\': \\'WAREHOUSE_CENTRAL\\', \\'destination_location\\': \\'S004\\', \\'notes\\': \\'Promotion replenishment - DELAYED 2 days\\'}, {\\'transaction_id\\': \\'INV017\\', \\'store_id\\': \\'S005\\', \\'store_name\\': \\'Southpark Store\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-03 00:00:00\\'), \\'transaction_type\\': \\'RECEIPT\\', \\'quantity\\': 130, \\'source_location\\': \\'WAREHOUSE_CENTRAL\\', \\'destination_location\\': \\'S005\\', \\'notes\\': \\'Promotion replenishment - DELAYED 2 days\\'}]\", \"tool_call_id\": \"call_sYOjgQQOLDGGRdOHqepklRqV\"}, {\"type\": \"ToolMessage\", \"content\": \"[{\\'transaction_id\\': \\'INV018\\', \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-04 00:00:00\\'), \\'transaction_type\\': \\'ADJUSTMENT\\', \\'quantity\\': -12, \\'source_location\\': \\'S002\\', \\'destination_location\\': \\'NONE\\', \\'notes\\': \\'Additional shrinkage found during stockout investigation\\'}]\", \"tool_call_id\": \"call_lW5r5cmAudraPDweJErE8X8g\"}, {\"type\": \"ToolMessage\", \"content\": \"[{\\'transaction_id\\': \\'INV011\\', \\'store_id\\': \\'S004\\', \\'store_name\\': \\'Northgate Center\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-01 00:00:00\\'), \\'transaction_type\\': \\'TRANSFER\\', \\'quantity\\': -50, \\'source_location\\': \\'S004\\', \\'destination_location\\': \\'S002\\', \\'notes\\': \\'Emergency transfer to Westside Mall\\'}, {\\'transaction_id\\': \\'INV012\\', \\'store_id\\': \\'S002\\', \\'store_name\\': \\'Westside Mall\\', \\'product_id\\': \\'P100\\', \\'product_name\\': \\'TechGadget Pro\\', \\'transaction_date\\': Timestamp(\\'2024-05-01 00:00:00\\'), \\'transaction_type\\': \\'TRANSFER\\', \\'quantity\\': 50, \\'source_location\\': \\'S004\\', \\'destination_location\\': \\'S002\\', \\'notes\\': \\'Emergency transfer from Northgate\\'}]\", \"tool_call_id\": \"call_SPOg2ITT8H23ZlsOCGBtrrlz\"}], \"root_cause\": {\"primary_root_causes\": [\"Unexpectedly high demand exceeding forecasted sales driven by promotional activities.\", \"Delays and disruptions in inventory replenishment and shipments causing stock shortages.\", \"Inefficient distribution and allocation leading to uneven stock levels across stores.\", \"Supply chain disruptions affecting timely product availability.\"], \"supporting_evidence\": {\"sales\": {\"promotion_period\": \"May 1 to May 7, 2024\", \"demand_surge\": \"Daily sales increased from 2-6 units pre-promo to 15-55 units during promo across all stores.\"}, \"inventory\": {\"high_sales_volume\": \"Stores sold 116 to 192 units during promo, exceeding normal forecast.\", \"replenishment_delay\": \"All stores had replenishment shipments delayed by 2 days during promo.\", \"emergency_transfers\": \"50 units transferred from Northgate Center to Westside Mall indicating uneven distribution.\", \"shrinkage\": \"12 units shrinkage recorded at Westside Mall during promo.\"}, \"cross_analysis\": {\"stockouts_cause\": \"High demand combined with replenishment delays and uneven stock allocation led to stockouts.\", \"supply_chain_impact\": \"Delayed shipments from central warehouse indicate supply chain issues impacting store availability.\"}}, \"contributing_factors\": [\"Promotional pricing and marketing driving sudden demand spikes.\", \"Stock mismanagement and shrinkage at store level exacerbating shortages.\", \"Operational strain possibly affecting inventory visibility due to delayed replenishments and emergency transfers.\"], \"timeline\": [{\"date\": \"2024-05-01\", \"event\": \"Promotion started, daily sales surged significantly across all stores.\"}, {\"date\": \"2024-05-01\", \"event\": \"Emergency stock transfer of 50 units from Northgate Center to Westside Mall to address uneven stock.\"}, {\"date\": \"2024-05-03\", \"event\": \"Replenishment shipments from central warehouse delayed by 2 days for all stores.\"}, {\"date\": \"2024-05-04\", \"event\": \"Shrinkage adjustment of 12 units recorded at Westside Mall.\"}, {\"date\": \"2024-05-07\", \"event\": \"Promotion ended.\"}], \"recommendations\": [\"Improve demand forecasting models to better anticipate promotional demand spikes.\", \"Enhance supply chain processes to reduce replenishment delays, including contingency plans for promotions.\", \"Optimize stock allocation and distribution to balance inventory across stores ahead of promotions.\", \"Implement tighter controls and monitoring to reduce shrinkage during high-demand periods.\", \"Strengthen inventory visibility and operational coordination during promotions to proactively manage stock levels.\"]}}]}', 'tool_call_id': 'call_jI3Gt11tvTS9m5GLVjIpNIGZ'}, {'type': 'AIMessage', 'content': '', 'tool_calls': [{'name': 'rca_report_agent_tool', 'args': {'root_cause': 'Unexpectedly high demand exceeding forecasted sales driven by promotional activities; delays and disruptions in inventory replenishment and shipments causing stock shortages; inefficient distribution and allocation leading to uneven stock levels across stores; supply chain disruptions affecting timely product availability.', 'reasoning': 'The analysis clearly shows that unexpectedly high promotional demand exceeded forecasts, stressing inventory. Delays in replenishment shipments and uneven stock distribution further contributed to stock shortages. Supply chain disruptions from central warehouse shipments delayed timely stock availability. Promotional activities caused demand spikes. Stock shrinkage at store level and operational strain during replenishment and emergency transfers worsened stockouts. Sales data demonstrated a sharp increase in demand during the promotion. Inventory data confirmed replenishment delays, emergency transfers, and shrinkage incidents, collectively supporting the root causes. The timeline highlights key events starting with the promotion launch and demand surge, followed by emergency stock transfers and replenishment delays, concluding with shrinkage findings and promotion end. Recommendations focus on improving demand forecasting, supply chain reliability, stock distribution, shrinkage control, and inventory visibility to prevent recurrence of similar stockouts.', 'user_id': '2', 'query_id': '2'}, 'id': 'call_jVJZhZ3w8lsycPEvF9NbaR4K'}]}, {'type': 'ToolMessage', 'content': 'Tool error: Please check your input and try again. (Expected dict, got [{\\'role\\': \\'system\\', \\'content\\': \\'\\\\nYou are an expert supply chain and demand planning analyst.\\\\n\\\\nCreate a professional Root Cause Analysis Report.\\\\n\\\\nAudience:\\\\n- Demand Planning\\\\n- Inventory Management\\\\n- Supply Chain Teams\\\\n\\\\nRequirements:\\\\n- Clear structured sections\\\\n- Bullet points where appropriate\\\\n- No JSON, no code\\\\n- Pure narrative report\\\\n\\\\nThe report MUST include:\\\\n- Executive Summary\\\\n- Primary Root Cause(s)\\\\n- Supporting Evidence\\\\n- Contributing Factors\\\\n- Key Data Points\\\\n- Timeline of Events\\\\n- Recommendations\\\\n- Final Conclusion\\\\n\\\\nTone:\\\\nAnalytical, data-driven, formal, concise.\\\\n\\'}, {\\'role\\': \\'user\\', \\'content\\': \\'\\\\nUse the following structured RCA output:\\\\n\\\\n\"Unexpectedly high demand exceeding forecasted sales driven by promotional activities; delays and disruptions in inventory replenishment and shipments causing stock shortages; inefficient distribution and allocation leading to uneven stock levels across stores; supply chain disruptions affecting timely product availability.\"\\\\n\"The analysis clearly shows that unexpectedly high promotional demand exceeded forecasts, stressing inventory. Delays in replenishment shipments and uneven stock distribution further contributed to stock shortages. Supply chain disruptions from central warehouse shipments delayed timely stock availability. Promotional activities caused demand spikes. Stock shrinkage at store level and operational strain during replenishment and emergency transfers worsened stockouts. Sales data demonstrated a sharp increase in demand during the promotion. Inventory data confirmed replenishment delays, emergency transfers, and shrinkage incidents, collectively supporting the root causes. The timeline highlights key events starting with the promotion launch and demand surge, followed by emergency stock transfers and replenishment delays, concluding with shrinkage findings and promotion end. Recommendations focus on improving demand forecasting, supply chain reliability, stock distribution, shrinkage control, and inventory visibility to prevent recurrence of similar stockouts.\"\\\\n\\'}]\\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/INVALID_GRAPH_NODE_RETURN_VALUE)', 'tool_call_id': 'call_jVJZhZ3w8lsycPEvF9NbaR4K'}]}\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "----------------------------------------------------------------------\n",
            "You      : exit\n",
            "episodic reflection === {'context_tags': ['inventory_stockout', 'promotional_demand_spike', 'replenishment_delay', 'stock_allocation_imbalance'], 'conversation_summary': 'Resolved store stockouts during a promotional period by identifying unexpectedly high demand, replenishment delays, uneven stock distribution, and shrinkage as root causes.', 'what_worked': 'Combining sales and inventory data analysis with timeline reconstruction to validate multiple contributing factors and pinpoint key operational failures.', 'what_to_avoid': 'Ignoring the combined impact of demand spikes and supply chain delays or overlooking stock mismanagement and allocation issues during promotions.', 'conversation': \"HUMAN: Investigate why stores are experiencing stockouts during the TechGadget Pro promotion\\nAI: There seems to be a technical issue preventing me from using some tools right now. However, I can still help by outlining common plausible root causes for stockouts during a promotion like TechGadget Pro:\\n\\n1. Unexpectedly high demand exceeding forecasted sales.\\n2. Delays or issues in inventory replenishment or shipments.\\n3. Distribution or allocation problems causing uneven stock levels across stores.\\n4. Point-of-sale or inventory system errors causing inaccurate stock visibility.\\n5. Supply chain disruptions affecting product availability.\\n6. Promotional pricing or marketing driving demand spikes.\\n7. Stock mismanagement or shrinkage at stores.\\n\\nPlease let me know if you want me to analyze available sales or inventory data to further validate these or if you have access to specific data sources I could help you interpret.\\nHUMAN: hi\\nAI: Hello! How can I assist you today?\\nHUMAN: Investigate why stores are facing stockouts\\nAI: It seems there is a technical issue preventing me from using some tools right now. However, I can help by outlining common plausible root causes for stockouts in stores:\\n\\n1. Unexpectedly high demand exceeding forecasted sales.\\n2. Delays or issues in inventory replenishment or shipments.\\n3. Distribution or allocation problems causing uneven stock levels across stores.\\n4. Point-of-sale or inventory system errors causing inaccurate stock visibility.\\n5. Supply chain disruptions affecting product availability.\\n6. Promotional pricing or marketing driving demand spikes.\\n7. Stock mismanagement or shrinkage at stores.\\n\\nIf you have specific sales or inventory data available, I can help you interpret it to further validate these causes. Please let me know how you'd like to proceed.\\nHUMAN: investigate why store facing stockouts\\nAI: It appears there is a technical issue preventing me from using some analysis tools right now. However, I can still help by outlining common plausible root causes for store stockouts based on typical scenarios:\\n\\n1. Unexpectedly high demand exceeding forecasted sales.\\n2. Delays or issues in inventory replenishment or shipments.\\n3. Distribution or allocation problems causing uneven stock levels across stores.\\n4. Point-of-sale or inventory system errors causing inaccurate stock visibility.\\n5. Supply chain disruptions affecting product availability.\\n6. Promotional pricing or marketing driving demand spikes.\\n7. Stock mismanagement or shrinkage at stores.\\n\\nIf you have specific sales or inventory data available, you can share it with me, and I can help you interpret it to further validate these causes. Please let me know how you'd like to proceed.\\nHUMAN: Investigate why stores are expeciecning stockouts\\nAI: Here is the Root Cause Analysis report for the stockouts experienced by stores:\\n\\nExecutive Summary:\\nStores experienced stockouts primarily due to unexpectedly high demand driven by a promotional event, combined with delays in inventory replenishment and disruptions in supply chain operations. Uneven stock allocation across stores and additional stock shrinkage further exacerbated shortages.\\n\\nPrimary Root Causes:\\n- Unexpectedly high demand exceeding forecasted sales caused by promotional activities.\\n- Delays and disruptions in inventory replenishment and shipments leading to stock shortages.\\n- Inefficient distribution and allocation resulting in uneven stock levels across stores.\\n- Supply chain disruptions affecting timely product availability.\\n\\nSupporting Evidence:\\n- Sales data showed daily sales surged from an average of 2-6 units per store before the promotion to 15-55 units during the promotional period (May 1 to May 7, 2024).\\n- Inventory data revealed replenishment shipments to all stores were delayed by 2 days during the promotion.\\n- An emergency transfer of 50 units from one store to another highlighted uneven stock distribution.\\n- Stock shrinkage of 12 units was recorded at one store during the promotional period.\\n- No evidence of point-of-sale or inventory system errors was found.\\n\\nContributing Factors:\\n- Promotional pricing and marketing campaigns driving sudden spikes in demand.\\n- Stock mismanagement and shrinkage at the store level.\\n- Operational strain impacting inventory visibility during periods of delayed replenishment and emergency stock transfers.\\n\\nKey Data Points:\\n- Promotion period: May 1 to May 7, 2024.\\n- Sales volume during promotion ranged from 116 to 192 units per store.\\n- Replenishment shipment delays of 2 days across all stores.\\n- Emergency stock transfer of 50 units between stores.\\n- Recorded shrinkage of 12 units at one store.\\n\\nTimeline of Events:\\n- May 1, 2024: Promotion launched; daily sales surged significantly.\\n- May 1, 2024: Emergency stock transfer of 50 units between stores.\\n- May 3, 2024: Replenishment shipments delayed by 2 days.\\n- May 4, 2024: Shrinkage adjustment of 12 units recorded.\\n- May 7, 2024: Promotion ended.\\n\\nRecommendations:\\n- Improve demand forecasting models to better anticipate promotional demand spikes.\\n- Enhance supply chain processes to minimize replenishment delays and establish contingency plans for promotional periods.\\n- Optimize stock allocation and distribution to ensure balanced inventory across stores before promotions.\\n- Implement tighter controls and monitoring to reduce shrinkage during high-demand periods.\\n- Strengthen inventory visibility and operational coordination during promotions for proactive stock management.\\n\\nFinal Conclusion:\\nThe stockouts experienced by stores during the promotional period were driven by a combination of unexpectedly high demand and supply chain challenges, including replenishment delays and uneven stock distribution. Addressing these root causes with improved forecasting, supply chain resilience, and inventory management practices will help prevent similar issues in future promotions.\"}\n",
            "== Conversation Stored in Episodic Memory ==\n",
            "semantic_reflection === {'semantic_fact': 'When conducting root cause analysis of store stockouts during promotions, a comprehensive approach that includes listing common plausible causes and combining multiple data sources (such as sales, inventory, and supply timelines) improves hypothesis generation and validation, especially when direct data access or technical tools are limited.', 'applicable_context': ['root cause analysis', 'stockouts', 'promotions', 'data limitations', 'hypothesis generation'], 'confidence': 'high'}\n",
            "== Conversation Stored in Semantic Memory ==\n",
            "procedural_reflection === {'procedure_name': 'Root Cause Analysis (RCA) for Retail Stockouts During Promotions', 'applicable_when': 'Used when investigating causes of stockouts in retail stores, especially during promotional events or unusual demand spikes.', 'steps': ['Identify and list common plausible root causes of stockouts (e.g., high demand, replenishment delays, distribution issues, system errors, supply chain disruptions, promotional effects, stock mismanagement).', 'Gather relevant data sources including sales data, inventory data, replenishment shipment records, stock transfer logs, and shrinkage reports.', 'Analyze sales data to detect demand surges or deviations from forecasted sales during the period of interest.', 'Review inventory replenishment and shipment timelines to identify delays or disruptions.', 'Examine stock distribution and allocation across stores for uneven stock levels or emergency transfers.', 'Check for point-of-sale or inventory system errors that might affect stock visibility or reporting accuracy.', 'Assess stock mismanagement or shrinkage incidents during the period.', 'Construct a timeline of events correlating demand spikes, inventory movements, shipment delays, and stock adjustments.', 'Synthesize findings into root causes and contributing factors supported by data evidence.', 'Develop actionable recommendations based on identified root causes to improve demand forecasting, supply chain operations, stock allocation, shrinkage controls, and inventory visibility.', 'Compile a comprehensive RCA report including executive summary, primary root causes, supporting evidence, contributing factors, key data points, timeline, recommendations, and final conclusions.'], 'tool_heuristics': ['Use sales and inventory data analysis tools when data sources are available and accessible.', 'If technical issues prevent use of automated tools, begin with expert knowledge and common root cause frameworks to guide manual analysis.', 'Prioritize analysis of demand data first to confirm if unexpectedly high demand is a primary factor.', 'Follow with supply chain and inventory replenishment data to identify operational delays or disruptions.', 'Use stock transfer and shrinkage data to explain uneven distribution or losses.', 'Validate findings through cross-referencing multiple data sources to ensure accuracy.', 'When system errors are suspected, prioritize point-of-sale and inventory system log checks.', 'Adjust analysis sequence dynamically based on data availability and technical constraints.']}\n",
            "== Conversation Stored in procedural Memory ==\n",
            "\n",
            "--------------------------------------------------------------------------\n",
            "memory inspector\n",
            "--------------------------------------------------------------------------\n",
            "{\n",
            "  \"episodic\": [\n",
            "    {\n",
            "      \"key\": \"episodic_rca_7f480ac83f274396b596028e52c9c846\",\n",
            "      \"confidence\": null,\n",
            "      \"usefulness\": 0,\n",
            "      \"summary\": \"Outlined common root causes for store stockouts during a high-demand product promotion despite limited data access.\"\n",
            "    },\n",
            "    {\n",
            "      \"key\": \"episodic_rca_0c377e473d4449d08beea78786008886\",\n",
            "      \"confidence\": null,\n",
            "      \"usefulness\": 0,\n",
            "      \"summary\": \"Outlined common root causes for store stockouts during a product promotion despite technical tool limitations.\"\n",
            "    },\n",
            "    {\n",
            "      \"key\": \"episodic_rca_62548a67dbc74591b7f0ad6f880be034\",\n",
            "      \"confidence\": null,\n",
            "      \"usefulness\": 0,\n",
            "      \"summary\": \"Outlined common root causes for store stockouts during a promotion despite technical tool limitations.\"\n",
            "    },\n",
            "    {\n",
            "      \"key\": \"episodic_rca_4775de77cc8b4dad960d8f7807189208\",\n",
            "      \"confidence\": null,\n",
            "      \"usefulness\": 0,\n",
            "      \"summary\": \"Resolved store stockouts during a promotional period by identifying unexpectedly high demand, replenishment delays, uneven stock distribution, and shrinkage as root causes.\"\n",
            "    }\n",
            "  ],\n",
            "  \"procedural\": [\n",
            "    {\n",
            "      \"key\": \"procedural_rca_2cc411a8ad3e493a8030e46a6a054141\",\n",
            "      \"confidence\": null,\n",
            "      \"usefulness\": 0,\n",
            "      \"summary\": \"RCA for Stockouts during Promotions\"\n",
            "    },\n",
            "    {\n",
            "      \"key\": \"procedural_rca_8e9e39ecfee14b75b4252261be44e9f1\",\n",
            "      \"confidence\": null,\n",
            "      \"usefulness\": 0,\n",
            "      \"summary\": \"RCA for Stockouts During Promotions\"\n",
            "    },\n",
            "    {\n",
            "      \"key\": \"procedural_rca_37eaf061b8cc41b29d219ab8624b0ca7\",\n",
            "      \"confidence\": null,\n",
            "      \"usefulness\": 0,\n",
            "      \"summary\": \"Root Cause Analysis for Store Stockouts\"\n",
            "    },\n",
            "    {\n",
            "      \"key\": \"procedural_rca_80934b44a9514d85999f32405338fe50\",\n",
            "      \"confidence\": null,\n",
            "      \"usefulness\": 0,\n",
            "      \"summary\": \"Root Cause Analysis (RCA) for Retail Stockouts During Promotions\"\n",
            "    }\n",
            "  ],\n",
            "  \"semantic\": [\n",
            "    {\n",
            "      \"key\": \"semantic_da0ebec0c33d4c5abc7d087f4f21087d\",\n",
            "      \"confidence\": \"high\",\n",
            "      \"usefulness\": 2,\n",
            "      \"summary\": \"When direct access to critical sales or inventory data is unavailable during root cause analysis, generating and categorizing plausible common causes enables effective hypothesis generation and guides targeted data validation.\"\n",
            "    },\n",
            "    {\n",
            "      \"key\": \"semantic_f256ac96b648404fa88e5f06f17f127e\",\n",
            "      \"confidence\": \"high\",\n",
            "      \"usefulness\": 1,\n",
            "      \"summary\": \"When conducting root cause analysis of store stockouts during promotions, a comprehensive approach that includes listing common plausible causes and combining multiple data sources (such as sales, inventory, and supply timelines) improves hypothesis generation and validation, especially when direct data access or technical tools are limited.\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            " Exiting RCA chatbot.\n"
          ]
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "graph = StateGraph(RCAState)\n",
        "\n",
        "graph.add_node(\"orchestration_agent\", orchestration_agent)\n",
        "graph.set_entry_point(\"orchestration_agent\")\n",
        "\n",
        "app = graph.compile(checkpointer=checkpointer, store=store)\n",
        "\n",
        "print(\"\\n RCA Chatbot (type 'exit' to quit)\\n\")\n",
        "DEFAULT_USER_ID = \"2\"\n",
        "DEFAULT_QUERY_ID = \"2\"\n",
        "# ----- chat loop -----\n",
        "while True:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    #user_id = input(f\"User ID     [{DEFAULT_USER_ID}] : \").strip()\n",
        "    #query_id = input(f\"Query ID   [{DEFAULT_QUERY_ID}] : \").strip()\n",
        "\n",
        "    user_id = DEFAULT_USER_ID\n",
        "    query_id = DEFAULT_QUERY_ID\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "    user_input = input(\"You      : \").strip()\n",
        "\n",
        "    if user_input.lower() in {\"exit\", \"quit\"}:\n",
        "\n",
        "        add_episodic_memory(rca_state, config)\n",
        "        print(\"== Conversation Stored in Episodic Memory ==\")\n",
        "\n",
        "        build_semantic_memory(\n",
        "            user_id=config[\"configurable\"][\"user_id\"],\n",
        "            query=user_input,\n",
        "            store=store,\n",
        "            llm=get_llm_model()\n",
        "        )\n",
        "        print(\"== Conversation Stored in Semantic Memory ==\")\n",
        "\n",
        "        add_procedural_memory(rca_state, config)\n",
        "        print(\"== Conversation Stored in procedural Memory ==\")\n",
        "\n",
        "\n",
        "        #Increment usefulness after a successful RCA\n",
        "        used_semantic = semantic_recall(\n",
        "            rca_state[\"task\"], store, config\n",
        "        )\n",
        "        mark_memory_useful(used_semantic)\n",
        "\n",
        "        print(json.dumps(\n",
        "            inspect_memory(store, user_id),\n",
        "            indent=2\n",
        "        ))\n",
        "        print(\"\\n Exiting RCA chatbot.\")\n",
        "        break\n",
        "\n",
        "    # build config dynamically\n",
        "    config = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": user_id,\n",
        "            \"thread_id\": query_id\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # update task\n",
        "    rca_state: RCAState = {\n",
        "        \"task\": user_input,\n",
        "        \"output\": \"\",\n",
        "        \"trace\": []\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\" RCA Bot is thinking...\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # invoke graph\n",
        "    rca_state = app.invoke(rca_state, config)\n",
        "\n",
        "    print(\"\\n RCA Bot Answer\")\n",
        "    print(\"-\" * 70)\n",
        "    print(rca_state.get(\"output\", \"No response generated\"))\n",
        "    print(rca_state.get(\"trace\", \"No trace generated\"))\n",
        "    print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOOL_TO_AGENT = {\n",
        "    \"hypothesis_agent_tool\": \"HypothesisAgent\",\n",
        "    \"sales_analysis_agent_tool\": \"SalesAnalysisAgent\",\n",
        "    \"inventory_analysis_agent_tool\": \"InventoryAnalysisAgent\",\n",
        "    \"hypothesis_validation_agent_tool\": \"HypothesisValidationAgent\",\n",
        "    \"root_cause_analysis_agent_tool\": \"RootCauseAgent\",\n",
        "    \"write_todos\": \"OrchestrationAgent\"\n",
        "}\n",
        "\n",
        "def flatten_trace(result: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Flattens orchestrator-driven traces into logical agent steps.\n",
        "    \"\"\"\n",
        "    flat = []\n",
        "\n",
        "    for msg in result.get(\"trace\", []):\n",
        "        # AIMessage with tool calls\n",
        "        if msg.get(\"tool_calls\"):\n",
        "            for call in msg[\"tool_calls\"]:\n",
        "                agent = TOOL_TO_AGENT.get(call[\"name\"], call[\"name\"])\n",
        "                flat.append({\n",
        "                    \"agent\": agent,\n",
        "                    \"tool\": call[\"name\"],\n",
        "                    \"args\": call.get(\"args\", {}),\n",
        "                    \"call_id\": call.get(\"id\")\n",
        "                })\n",
        "\n",
        "        # ToolMessage with content\n",
        "        if msg.get(\"type\") == \"ToolMessage\":\n",
        "            flat.append({\n",
        "                \"agent\": \"ToolResult\",\n",
        "                \"content\": msg.get(\"content\"),\n",
        "                \"tool_call_id\": msg.get(\"tool_call_id\")\n",
        "            })\n",
        "\n",
        "    return flat\n"
      ],
      "metadata": {
        "id": "yHo7IHJKA6m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_root_cause(result: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Extract root cause output from root_cause_analysis_agent_tool\n",
        "    \"\"\"\n",
        "    for step in result.get(\"trace\", []):\n",
        "        for call in step.get(\"tool_calls\", []):\n",
        "            if call.get(\"name\") == \"root_cause_analysis_agent_tool\":\n",
        "                return call.get(\"output\", {})\n",
        "    return {}\n"
      ],
      "metadata": {
        "id": "nMsnVLioCn11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "husIY7l4JbDY"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "RCA Agent Evaluation Framework\n",
        "=============================\n",
        "\n",
        "This file contains:\n",
        "1. Gold RCA dataset design\n",
        "2. End-to-end evaluation runner\n",
        "3. Memory-on vs memory-off comparison\n",
        "4. Learning-over-time metrics\n",
        "\n",
        "Designed specifically for your Agentic RCA system.\n",
        "\"\"\"\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 1. GOLD RCA DATASET\n",
        "# --------------------------------------------------\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "\n",
        "@dataclass\n",
        "class GoldRCACase:\n",
        "    case_id: str\n",
        "    task: str\n",
        "    expected_root_causes: List[str]\n",
        "    gold_hypotheses: List[str]\n",
        "    must_use_agents: List[str]\n",
        "    forbidden_root_causes: List[str]\n",
        "\n",
        "GOLD_RCA_DATASET: List[GoldRCACase] = [\n",
        "    GoldRCACase(\n",
        "        case_id=\"PROMO_STOCKOUT_01\",\n",
        "        task=\"Why did Store S003 face stockouts during the Diwali promotion?\",\n",
        "        expected_root_causes=[\n",
        "            \"Delayed replenishment\",\n",
        "            \"Promo uplift underestimated\"\n",
        "        ],\n",
        "        gold_hypotheses=[\n",
        "            \"Demand spike due to promotion\",\n",
        "            \"Delayed replenishment\",\n",
        "            \"Inventory transfer delay\",\n",
        "            \"Forecast underestimation\"\n",
        "        ],\n",
        "        must_use_agents=[\n",
        "            \"HypothesisAgent\",\n",
        "            \"SalesAnalysisAgent\",\n",
        "            \"InventoryAnalysisAgent\",\n",
        "            \"HypothesisValidationAgent\"\n",
        "        ],\n",
        "        forbidden_root_causes=[\n",
        "            \"System outage\",\n",
        "            \"Pricing error\"\n",
        "        ]\n",
        "    ),\n",
        "\n",
        "    GoldRCACase(\n",
        "        case_id=\"SALES_DROP_02\",\n",
        "        task=\"Why did sales drop in the North region despite stable inventory?\",\n",
        "        expected_root_causes=[\n",
        "            \"Pricing mismatch\",\n",
        "            \"Local competition impact\"\n",
        "        ],\n",
        "        gold_hypotheses=[\n",
        "            \"Price increase\",\n",
        "            \"Competitive promotion\",\n",
        "            \"Demand elasticity change\",\n",
        "            \"Assortment mismatch\"\n",
        "        ],\n",
        "        must_use_agents=[\n",
        "            \"HypothesisAgent\",\n",
        "            \"SalesAnalysisAgent\",\n",
        "            \"HypothesisValidationAgent\"\n",
        "        ],\n",
        "        forbidden_root_causes=[\n",
        "            \"Inventory stockout\",\n",
        "            \"Warehouse delay\"\n",
        "        ]\n",
        "    )\n",
        "]\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 2. UTILITY FUNCTIONS\n",
        "# --------------------------------------------------\n",
        "\n",
        "def normalize(text: str) -> str:\n",
        "    return text.lower().strip()\n",
        "\n",
        "\n",
        "def semantic_match(a: str, b: str) -> bool:\n",
        "    \"\"\"\n",
        "    Lightweight semantic match.\n",
        "    You can replace this with embeddings later.\n",
        "    \"\"\"\n",
        "    a, b = normalize(a), normalize(b)\n",
        "    return a in b or b in a\n",
        "\n",
        "\n",
        "def count_semantic_matches(predicted: List[str], gold: List[str]) -> int:\n",
        "    count = 0\n",
        "    for g in gold:\n",
        "        if any(semantic_match(p, g) for p in predicted):\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 3. CORE EVAL METRICS\n",
        "# --------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class EvalScores:\n",
        "    precision: float\n",
        "    recall: float\n",
        "    hypothesis_coverage: float\n",
        "    evidence_score: float\n",
        "    process_compliance: bool\n",
        "    forbidden_penalty: bool\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 4. TRACE-BASED CHECKS\n",
        "# --------------------------------------------------\n",
        "\n",
        "def check_process_order(trace: List[Dict[str, Any]], required_agents: List[str]) -> bool:\n",
        "    executed = {t[\"agent\"] for t in trace}\n",
        "    return all(agent in executed for agent in required_agents)\n",
        "\n",
        "\n",
        "def evidence_backed(validated: Dict[str, bool], trace: List[Dict[str, Any]]) -> float:\n",
        "    if not validated:\n",
        "        return 0.0\n",
        "\n",
        "    evidence_agents = {\n",
        "        \"SalesAnalysisAgent\",\n",
        "        \"InventoryAnalysisAgent\"\n",
        "    }\n",
        "\n",
        "    used_agents = {t[\"agent\"] for t in trace}\n",
        "    has_evidence = evidence_agents.intersection(used_agents)\n",
        "\n",
        "    supported = sum(1 for v in validated.values() if v and has_evidence)\n",
        "\n",
        "    return supported / max(len(validated), 1)\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 5. SINGLE RCA CASE EVAL\n",
        "# --------------------------------------------------\n",
        "\n",
        "def evaluate_single_case(\n",
        "    gold: GoldRCACase,\n",
        "    rca_output: Dict[str, Any]\n",
        ") -> EvalScores:\n",
        "\n",
        "    trace = flatten_trace(rca_output)\n",
        "\n",
        "    root_causes = rca_output[\"root_cause\"][\"primary_root_causes\"]\n",
        "    hypotheses = rca_output.get(\"hypotheses\", [])\n",
        "    validated = rca_output.get(\"validated\", {})\n",
        "\n",
        "    matched = count_semantic_matches(root_causes, gold.expected_root_causes)\n",
        "    precision = matched / max(len(root_causes), 1)\n",
        "    recall = matched / max(len(gold.expected_root_causes), 1)\n",
        "\n",
        "    coverage = count_semantic_matches(hypotheses, gold.gold_hypotheses)\n",
        "    hypothesis_coverage = coverage / max(len(gold.gold_hypotheses), 1)\n",
        "\n",
        "    evidence = evidence_backed(validated, trace)\n",
        "\n",
        "    process_ok = check_process_order(trace, gold.must_use_agents)\n",
        "\n",
        "    forbidden_penalty = any(\n",
        "        semantic_match(rc, f)\n",
        "        for rc in root_causes\n",
        "        for f in gold.forbidden_root_causes\n",
        "    )\n",
        "\n",
        "    return EvalScores(\n",
        "        precision=precision,\n",
        "        recall=recall,\n",
        "        hypothesis_coverage=hypothesis_coverage,\n",
        "        evidence_score=evidence,\n",
        "        process_compliance=process_ok,\n",
        "        forbidden_penalty=forbidden_penalty\n",
        "    )\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 6. MEMORY ON vs OFF EVAL\n",
        "# --------------------------------------------------\n",
        "\n",
        "def run_memory_ablation(\n",
        "    case: GoldRCACase,\n",
        "    run_with_memory,\n",
        "    run_without_memory\n",
        ") -> Dict[str, EvalScores]:\n",
        "    \"\"\"\n",
        "    run_with_memory / run_without_memory are callables\n",
        "    that return RCA output dicts.\n",
        "    \"\"\"\n",
        "\n",
        "    out_mem = run_with_memory(case.task)\n",
        "    out_nomem = run_without_memory(case.task)\n",
        "\n",
        "    return {\n",
        "        \"with_memory\": evaluate_single_case(case, out_mem),\n",
        "        \"without_memory\": evaluate_single_case(case, out_nomem)\n",
        "    }\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 7. LEARNING OVER TIME EVAL\n",
        "# --------------------------------------------------\n",
        "\n",
        "def learning_curve(cases: List[GoldRCACase], run_fn) -> List[float]:\n",
        "    \"\"\"\n",
        "    Measures recall improvement over repeated similar RCA runs.\n",
        "    \"\"\"\n",
        "    recalls = []\n",
        "    for c in cases:\n",
        "        out = run_fn(c.task)\n",
        "        score = evaluate_single_case(c, out)\n",
        "        recalls.append(score.recall)\n",
        "    return recalls\n",
        "\n",
        "# --------------------------------------------------\n",
        "# END\n",
        "# --------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_trace(result):\n",
        "    traces = []\n",
        "    if \"trace\" in result and isinstance(result[\"trace\"], dict):\n",
        "        traces.append(result[\"trace\"])\n",
        "    return traces\n",
        "\n",
        "\n",
        "def extract_hypotheses(result: dict) -> list:\n",
        "    for step in result.get(\"trace\", []):\n",
        "        if step.get(\"agent\") == \"HypothesisAgent\":\n",
        "            return step.get(\"hypotheses\", [])\n",
        "    return []\n",
        "\n",
        "\n",
        "\n",
        "def extract_validated(result: dict) -> dict:\n",
        "    for step in result.get(\"trace\", []):\n",
        "        if step.get(\"agent\") == \"HypothesisValidationAgent\":\n",
        "            return step.get(\"details\", {}).get(\"validated\", {})\n",
        "    return {}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "839Ms7vfDrs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_trace(trace):\n",
        "    \"\"\"\n",
        "    Ensures trace is always List[Dict]\n",
        "    \"\"\"\n",
        "    if trace is None:\n",
        "        return []\n",
        "\n",
        "    if isinstance(trace, dict):\n",
        "        return [trace]\n",
        "\n",
        "    if isinstance(trace, list):\n",
        "        return [t for t in trace if isinstance(t, dict)]\n",
        "\n",
        "    return []\n"
      ],
      "metadata": {
        "id": "En8JiJgWGM-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rca_with_memory(task: str) -> dict:\n",
        "    config = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": \"eval_user\",\n",
        "            \"thread_id\": \"eval_thread\",\n",
        "            \"memory_enabled\": True\n",
        "        }\n",
        "    }\n",
        "\n",
        "    rca_state = {\n",
        "        \"task\": task,\n",
        "        \"trace\": []\n",
        "    }\n",
        "\n",
        "    result = app.invoke(rca_state, config)\n",
        "\n",
        "    normalized_trace = normalize_trace(result.get(\"trace\"))\n",
        "\n",
        "    return {\n",
        "        \"root_cause\": extract_root_cause({\"trace\": normalized_trace}),\n",
        "        \"hypotheses\": extract_hypotheses({\"trace\": normalized_trace}),\n",
        "        \"validated\": extract_validated({\"trace\": normalized_trace}),\n",
        "        \"trace\": normalized_trace\n",
        "    }\n"
      ],
      "metadata": {
        "id": "UrcOIMS7DJSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rca_without_memory(task: str) -> dict:\n",
        "    config = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": \"eval_user_nomem\",\n",
        "            \"thread_id\": \"eval_thread_nomem\",\n",
        "            \"memory_enabled\": False\n",
        "        }\n",
        "    }\n",
        "\n",
        "    empty_state = {\n",
        "        \"task\": task,\n",
        "        \"trace\": []\n",
        "    }\n",
        "\n",
        "    result = app.invoke(empty_state, config)\n",
        "\n",
        "    return {\n",
        "        \"root_cause\": extract_root_cause(result),\n",
        "        \"hypotheses\": extract_hypotheses(result),\n",
        "        \"validated\": extract_validated(result),\n",
        "        \"trace\": result.get(\"trace\", [])\n",
        "    }\n"
      ],
      "metadata": {
        "id": "yB_H6jSEDdws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gold_case = GOLD_RCA_DATASET[0]\n",
        "\n",
        "output = run_rca_with_memory(gold_case.task)\n",
        "\n",
        "scores = evaluate_single_case(gold_case, output)\n",
        "\n",
        "print(scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "collapsed": true,
        "id": "jCOCxTmvDojI",
        "outputId": "04803f66-7861-4106-e0e3-711850b4a08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Empty History***\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'name'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-867964678.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_rca_with_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_single_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_case\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3635039825.py\u001b[0m in \u001b[0;36mevaluate_single_case\u001b[0;34m(gold, rca_output)\u001b[0m\n\u001b[1;32m    153\u001b[0m ) -> EvalScores:\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrca_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mroot_causes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrca_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root_cause\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"primary_root_causes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-674757743.py\u001b[0m in \u001b[0;36mflatten_trace\u001b[0;34m(result)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tool_calls\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcall\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tool_calls\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOOL_TO_AGENT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 flat.append({\n\u001b[1;32m     22\u001b[0m                     \u001b[0;34m\"agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'name'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSDEWDm_D107"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}